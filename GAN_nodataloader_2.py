import itertools

import numpy as np
from timeit import default_timer as timer

import sklearn
import scipy.sparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils
import collections, functools, operator
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from build_classifier import train_target_model
import hiddenlayer as h1
from sklearn import metrics

class Generator_using_malware_and_noise(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_malware_and_noise, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))
        return torch.max(x_and_example[0], x)


class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu_(self.map1(x))
        x=torch.relu_(self.map2(x))
        return F.softmax(x , dim=1)
    
class GAN():
    def __init__(self, input_size, hidden_size, noise_size, device):
        self.original_data_size = 0
        self.noise_size = noise_size
        self.input_size = input_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=2).to(device)
        self.G = Generator_using_malware_and_noise(input_size=input_size + self.noise_size, hidden_size=hidden_size,output_size=input_size).cuda()

        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.00001, betas=(0.9, 0.999))
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.00001, betas=(0.9, 0.999))

    def binarize(x):
        return torch.where(x > 0.5, torch.ones_like(x),
                           torch.zeros_like(x))

    def FNR(self, x , classifier):
        noise = np.random.uniform(0, 1, (x.shape[0], self.noise_size))
        #gen_examples = self.binarize\
        x_= (self.G([torch.from_numpy(x).float().cuda(), torch.from_numpy(noise).float().cuda()]))
        gen_examples = torch.where(x_> 0.5, torch.ones_like(x_),
                    torch.zeros_like(x_))
        # if (sum(np.ones(gen_examples.shape[0]) - classifier.predict(gen_examples.cpu().detach().numpy())) != 0):
        true_label_for_gen_example = np.ones(gen_examples.shape[0])
        predicted_label_for_gen_example=classifier.predict(gen_examples.cpu().detach().numpy())
        # predicted_label_for_gen_example = np.hstack([np.zeros([predicted_label_for_gen_example.shape[0], 1]), np.ones([predicted_label_for_gen_example.shape[0], 1])])
        if (sum(true_label_for_gen_example - predicted_label_for_gen_example) >=1):
            # conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]),classifier.predict(gen_examples.cpu().detach().numpy()))      #.predict
            conf_matx = confusion_matrix(true_label_for_gen_example, predicted_label_for_gen_example)
            FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
        else:
            FNR = 0
        # y_pred=  classifier.predict(gen_examples.cpu().detach().numpy()) #.predict
        # y_pred=  classifier(gen_examples.cpu().detach().numpy())
        # y_test= np.ones(gen_examples.shape[0])
        # print(metrics.classification_report(y_test, y_pred, labels=[1, 0], target_names=['Malware', 'Goodware']))
        return FNR

    def train(self, features, classifier, device, epochs, data_original, batch_size=32):
        (xtrmal, ytrmal), (xtrben, ytrben), (xtsmal, xtsben), (ytsmal, ytsben) = data_original[0], data_original[1], \
                                                                         data_original[2], data_original[3]

        lb = preprocessing.LabelBinarizer()
        lb.fit([0, 1])
        for epoch in range(epochs):
            for step in range(xtrmal.shape[0] // batch_size):
                idx = np.random.randint(0, xtrmal.shape[0], batch_size)
                xmal_batch = torch.from_numpy(xtrmal[idx]).float().to(device)
                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                idx = np.random.randint(0, xmal_batch.shape[0], batch_size)
                xben_batch = xtrben[idx]
                yclassifierben_batch = classifier.predict(xben_batch)
                yclassifierben_batch = torch.from_numpy(np.hstack((1 - lb.transform(yclassifierben_batch), lb.transform(yclassifierben_batch)))).float().to(device)

                self.D.zero_grad()
                x =self.G([xmal_batch, noise.float().to(device)]).detach()

                gen_examples = torch.where(x > 0.5, torch.ones_like(x),
                                           torch.zeros_like(x))
                d_fake_decision = self.D(gen_examples)
                fake_data_lable = torch.cat((torch.zeros(d_fake_decision.shape[0] ,1).cuda() , torch.ones(d_fake_decision.shape[0],1).cuda()) , 1)
                d_loss_fake = self.criterion(d_fake_decision,fake_data_lable)
                d_real_decision = self.D(torch.from_numpy(xben_batch).float().to(device))
                d_loss_real = self.criterion(d_real_decision,yclassifierben_batch)
                d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)
                d_loss.backward(retain_graph=True)
                self.d_optimizer.step()



                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                self.G.zero_grad()
                g_fake_data = self.G([xmal_batch, noise.float().cuda()])
                dg_fake_decision = self.D(g_fake_data)
                g_desired_lable = torch.cat((torch.ones(d_fake_decision.shape[0] ,1).cuda() , torch.zeros(d_fake_decision.shape[0],1).cuda()) , 1)
                g_loss = nn.functional.mse_loss(dg_fake_decision, g_desired_lable)
                g_loss.backward(retain_graph=True)
                self.g_optimizer.step()

                torch.cuda.empty_cache()

            train_FNR = self.FNR(xtrmal, classifier)
            test_FNR = self.FNR(xtsmal, classifier)
            print("%d [D loss: %f] [G loss: %f] [train_FNR: %f] [test_FNR: %f] "
                  % (epoch, d_loss, g_loss, train_FNR, test_FNR))




