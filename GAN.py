import itertools

import numpy as np
from timeit import default_timer as timer

import sklearn
import scipy.sparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils
import collections, functools, operator
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from build_classifier import train_target_model
import hiddenlayer as h1
from sklearn import metrics



class Generator_using_malware_and_noise(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_malware_and_noise, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.relu_(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)
class Generator_using_noise_only(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_noise_only, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        #x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.relu(self.map1(x_and_example[1]))
        x = torch.relu(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)
class Generator_using_malware_only(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_malware_only, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        #x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x_and_example[0]))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)
class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu_(self.map1(x))
        x=torch.relu_(self.map2(x))
        return F.softmax(x , dim=1)

def spy_sparse2torch_sparse(data):
    """

    :param data: a scipy sparse csr matrix
    :return: a sparse torch tensor
    """
    samples=data.shape[0]
    features=data.shape[1]
    values=data.data
    coo_data=data.tocoo()
    indices=torch.LongTensor([coo_data.row,coo_data.col])
    t=torch.sparse.FloatTensor(indices,torch.from_numpy(values).float(),[samples,features]).to_dense()
    return t

def sampling(xtrain,ytrain,batch_size):
    class_sample_count = np.array(
        [len(np.where(ytrain == t)[0]) for t in np.unique(ytrain)])
    weight = 1. / class_sample_count
    samples_weight = []
    for t in range(len(ytrain) - 1):
        samples_weight.append(weight[int(ytrain[t])])
    sampler = data_utils.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))
    train = data_utils.TensorDataset(torch.from_numpy(xtrain), torch.from_numpy(ytrain))
    return data_utils.DataLoader(train, batch_size=batch_size, sampler=sampler, drop_last=True)

def binarize(x):
    return torch.where(x > 0.5, torch.ones_like(x),
                                          torch.zeros_like(x))
def check_added_features(x,features):
    added_features =[]
    for i in range(len(x)):
        unique, counts = np.unique(np.where(x.cpu().numpy()[i].astype(int) > 0, features[:, 2], np.zeros(features[:, 2].shape)),
            return_counts=True)
        unique=unique[1:]
        counts=counts[1:]
        added_features.append(dict(zip(unique, counts)))

    batch_added_features = dict(functools.reduce(operator.add, map(collections.Counter, added_features)))
    batch_added_features.update({n: batch_added_features[n] / len(x) for n in batch_added_features.keys()})
    return batch_added_features

def plot_added_featues(list_of_added_features):
    frams = pd.DataFrame(list_of_added_features)
    ax = frams.plot.bar(stacked=True)
    ax.set_xlabel('EPOCH')
    # for rowNum, row in frams.iterrows():
    #     ypos = 0
    #     featuer=0
    #     for val in row:
    #         if featuer!=6:
    #             ypos += val
    #             ax.text(rowNum, ypos , "{0:.2f}".format(val), color='black' ,ha='center')
    #         featuer+=1
    #     ypos = 0
    plt.title('Average Number of Added Features in Each Epoch')
    # chartBox = ax.get_position()
    # ax.set_position([chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])
    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.8), ncol=1)
    plt.show()

def check_lenght(x):
    if (len(x.shape) > 2):
        return x.reshape((x.shape[0] * x.shape[1]), x.shape[2])
    else :
        return x

def plot_grad_flow(named_parameters):
    ave_grads = []
    layers = []
    for n, p in named_parameters:
        if (p.requires_grad) and ("bias" not in n):
            layers.append(n)
            ave_grads.append(p.grad.abs().mean())
    plt.plot(ave_grads, alpha=0.3, color="b")
    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color="k")
    plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
    plt.xlim(left=0, right=len(ave_grads))
    plt.xlabel("Layers")
    plt.ylabel("average gradient")
    plt.title("Gradient flow")
    plt.grid(True)

def grad_changes(named_parameters):
    ave_grads = []
    layers = []
    bar={}
    for n, p in named_parameters:
        if (p.requires_grad) and ("bias" not in n):
            bar.update({n: np.asscalar(p.grad.abs().mean().cpu().detach().numpy())})
            # layers.append(n)
            # ave_grads.append(p.grad.abs().mean())
    return  bar
    # plt.plot(ave_grads, alpha=0.3, color="b")
    # plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color="k")
    # plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
    # plt.xlim(left=0, right=len(ave_grads))
    # plt.xlabel("Layers")
    # plt.ylabel("average gradient")
    # plt.title("Gradient flow")
    # plt.grid(True)

    # ave_grads = []
    # max_grads = []
    # layers = []
    # for n, p in named_parameters:
    #     if (p.requires_grad) and ("bias" not in n):
    #         layers.append(n)
    #         ave_grads.append(p.grad.abs().mean())
    #         max_grads.append(p.grad.abs().max())
    # plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="c")
    # plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")
    # plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color="k")
    # plt.xticks(range(0, len(ave_grads), 1), layers, rotation="vertical")
    # plt.xlim(left=0, right=len(ave_grads))
    # plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions
    # plt.xlabel("Layers")
    # plt.ylabel("average gradient")
    # plt.title("Gradient flow")
    # plt.grid(True)
    # plt.legend([Line2D([0], [0], color="c", lw=4),
    #             Line2D([0], [0], color="b", lw=4),
    #             Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])


class GAN():
    def __init__(self, input_size, hidden_size, noise_size , generator_type , losstype):
        self.otiginal_data_size = 0
        self.noise_size = noise_size
        self.input_size = input_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.criterion_ = nn.L1Loss()
        # self.criterion__ = nn.functional.mse_loss()

        self.losstype = losstype
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=2).cuda()
        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.00001, betas=(0.9, 0.999))
        if generator_type == 'Mal+Noise':
            self.G = Generator_using_malware_and_noise(input_size=input_size + self.noise_size, hidden_size=hidden_size,
                           output_size=input_size).cuda()
        if generator_type == 'Noise':
            self.G = Generator_using_noise_only(input_size=self.noise_size, hidden_size=hidden_size,
                                                       output_size=input_size).cuda()
        if generator_type == 'Mal':
            self.G = Generator_using_malware_only(input_size=self.input_size, hidden_size=hidden_size,
                                                output_size=input_size).cuda()
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.00001, betas=(0.9, 0.999))

    def FNR(self, x , classifier):
        noise = np.random.uniform(0, 1, (x.shape[0], self.noise_size))
        gen_examples = binarize(self.G([torch.from_numpy(x).float().cuda(), torch.from_numpy(noise).float().cuda()]))
        if (sum(np.ones(gen_examples.shape[0]) - classifier.predict(gen_examples.cpu().detach().numpy())) != 0):
        # true_label_for_gen_example = np.hstack([np.zeros([gen_examples.shape[0], 1]), np.ones([gen_examples.shape[0], 1])])
        # if (sum(sum(true_label_for_gen_example - classifier(gen_examples).cpu().detach().numpy())) >=1):
            conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]),classifier.predict(gen_examples.cpu().detach().numpy()))      #.predict
            # conf_matx = confusion_matrix(true_label_for_gen_example.argmax(axis=1), classifier(gen_examples).cpu().detach().numpy().argmax(axis=1))
            FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
        else:
            FNR = 0
        # y_pred=  classifier.predict(gen_examples.cpu().detach().numpy()) #.predict
        # y_pred=  classifier(gen_examples.cpu().detach().numpy())
        # y_test= np.ones(gen_examples.shape[0])
        # print(metrics.classification_report(y_test, y_pred, labels=[1, 0], target_names=['Malware', 'Goodware']))
        return FNR
    def train(self, features, classifier, device, epochs, data_original, batch_size=32, round=-1 , por=0.1 ):
    # def train(self, classifier, epochs, data_original, batch_size=32, round=-1 , por=0.1 ):
        c= h1.Canvas()
        hidden = h1.build_graph(self.D, torch.zeros([1,542]).cuda())
        hidden.save('./discriminator.png')

        hidden = h1.build_graph(self.G, torch.zeros([1, 642]).cuda())
        hidden.save('./generator.png')

        (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data_original[0], data_original[1], \
                                                                         data_original[2], data_original[3]

        xtrain = np.concatenate([xben, xmal])
        ytrain = np.concatenate([yben, ymal])

        #sampling for unbalanced data
        train_loader = sampling(xtrain,ytrain,batch_size)

        # classifier = train_target_model(classifier, train_loader, 100)

        print ('\nTRAINING GAN...\n')
        print('=============data============= ')
        print(
            'number of original malware = {0}\n'
            'number of original ben = {1}\n'
            'number of original test = {2}\n'
                .format(xmal.shape[0], xben.shape[0], xtsmal.shape[0]))
        print('==============================\n ')

        start_train = timer()
        Train_FNR, Test_FNR = [], []
        best_test_FNR, best_train_FNR = 0.0, 0.0

        self.gloss, self.dloss = [], []
        list_of_added_features = []
        g_grad_changes =[]
        lb = preprocessing.LabelBinarizer()
        lb.fit([0,1])

        for epoch in range(epochs):
            list_of_distortion = []
            batch_added_features=[]
            for local_batch, local_lable in train_loader:
            # for step in range(xtrain.shape[0] // batch_size):
                xmal_batch = check_lenght(local_batch[(local_lable != 0).nonzero()])
                xben_batch = check_lenght(local_batch[(local_lable == 0).nonzero()])

                # xmal_batch = check_lenght(xtrain[(ytrain != 0).nonzero()])
                # xben_batch = check_lenght(xtrain[(ytrain == 0).nonzero()])
                # xmal_batch = torch.from_numpy(xmal_batch)

                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                yclassifierben_batch = classifier.predict(xben_batch)
                # yclassifierben_batch = classifier(xben_batch.float().cuda())

                # xben_batch= torch.from_numpy(xben_batch)

                # Generate a batch of new malware examples
                self.D.zero_grad()
                # gen_examples =binarize(self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach())
                gen_examples =self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()

                # Check what features are added in this batch and add it to the list
                batch_added_features.append(check_added_features(binarize(gen_examples)-xmal_batch.cuda().float(), features))

                # ---------------------
                #  Train Discriminator
                # ---------------------
                d_fake_decision = self.D(gen_examples)
                fake_data_lable = torch.cat((torch.zeros(d_fake_decision.shape[0] ,1).cuda() , torch.ones(d_fake_decision.shape[0],1).cuda()) , 1)
                d_loss_fake = self.criterion(d_fake_decision,fake_data_lable)
                d_real_decision = self.D(xben_batch.float().cuda())
                real_lable = np.hstack((1-lb.transform(yclassifierben_batch) , lb.transform(yclassifierben_batch )) )
                d_loss_real = self.criterion(d_real_decision,torch.from_numpy(real_lable).float().cuda())
                # d_loss_real = self.criterion(d_real_decision,yclassifierben_batch.detach())
                # d_loss_real = self.criterion(d_real_decision,torch.zeros_like(d_real_decision))
                d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)
                d_loss_real.backward(retain_graph=True)
                d_loss_fake.backward(retain_graph=True)
                # plot_grad_flow(self.D.named_parameters())

                self.d_optimizer.step()

                # ---------------------
                #  Train Generator
                # ---------------------

                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                self.G.zero_grad()
                g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()])
                dg_fake_decision = self.D(g_fake_data)
                g_desired_lable = torch.cat((torch.ones(d_fake_decision.shape[0] ,1).cuda() , torch.zeros(d_fake_decision.shape[0],1).cuda()) , 1)
                g_loss_samples = nn.functional.mse_loss(dg_fake_decision, g_desired_lable)
                # g_loss_samples.reqiers_grad = True
                if self.losstype == 'limited_distortion':
                    orig_adv_dist=torch.sum(g_fake_data-xmal_batch.float().cuda(),1)
                    g_loss = g_loss_samples
                    list_of_distortion.append(orig_adv_dist.cpu().detach().numpy())
                # if self.losstype == 'unlimited_distorion':
                    # g_loss = g_loss_samples

                # g_loss_samples.backward(retain_graph=True)
                g_loss.backward(retain_graph=True)
                plot_grad_flow(self.G.named_parameters())
                g_grad_changes.append(grad_changes(self.G.named_parameters()))
                self.g_optimizer.step()

                torch.cuda.empty_cache()

            list_of_added_features.append(pd.DataFrame(batch_added_features).mean(axis=0).to_dict())
            self.gloss.append(g_loss)
            self.dloss.append(d_loss)

            # Compute attack success rate on train and test data
            train_FNR=self.FNR(xmal, classifier)
            test_FNR = self.FNR(xtsmal, classifier)


            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            Train_FNR.append(train_FNR)
            Test_FNR.append(test_FNR)
            if test_FNR > best_test_FNR:
                best_test_FNR = test_FNR
                print('saving mulgan weights at epoch: %d', epoch)

                print("[G loss: %f] [D loss: %f] \n" % (self.gloss[-1], self.dloss[-1]))
                torch.save(self.G,
                           '/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/malgan{0}.pt'.format(round))

            print("%d [D loss: %f] [G loss: %f] [train_FNR: %f] [test_FNR: %f] [distortion: %f]"
                  % (epoch, d_loss, g_loss, train_FNR, test_FNR,np.mean(list(itertools.chain(*list_of_distortion)))))
        end_train = timer()
        del g_loss, d_loss_real, d_loss_fake # d_loss

        print('\ntraining completed in %.3f seconds.\n' % (end_train - start_train))
        print('=============results ============= ')

        print(' attack success rate using train data: {0} \n'
              ' attack success rate using test data: {1}'.format(best_train_FNR, best_test_FNR))
        print('==============================\n ')

        plot_added_featues(list_of_added_features)
        plot_added_featues(g_grad_changes)


        # Plot losses
        plt.figure()
        plt.plot(range(len(self.gloss)), self.gloss, c='r', label='g_loss_rec', linewidth=2)
        plt.plot(range(len(self.dloss)), self.dloss, c='g', linestyle='--', label='d_loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('loss')
        plt.legend()
        plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/GAN_Epoch_loss({0}).png'.format(round))
        # plt.show()
        plt.close()

        # Plot TPR
        plt.figure()
        plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Training Set', linewidth=2)
        # plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Attack success rate', linewidth=2)
        plt.plot(range(len(Test_FNR)), Test_FNR, c='g', linestyle='--', label='Test Set', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('FNR')
        plt.legend()
        plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/Epoch_FNR({0}).png'.format(round))
        plt.show()
        plt.close()


        return [best_train_FNR, best_test_FNR, (end_train - start_train)]
        # return Train_FNR
