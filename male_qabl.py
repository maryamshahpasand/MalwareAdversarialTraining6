import numpy as np
import sys
import psutil
import os
import csv
import random
from timeit import default_timer as timer
import pickle

import torch
import torch.nn as nn
# from torchvision import transforms
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils

from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import linear_model, svm, tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score,precision_score

import collections, functools, operator
import sklearn
import scipy.sparse
import pandas as pd


from sklearn.metrics import roc_curve, auc

class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=7, verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''Saves model when validation loss decrease.'''
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), 'checkpoint.pt')
        self.val_loss_min = val_loss
def cpuStats():
    print(sys.version)
    print(psutil.cpu_percent())
    print(psutil.virtual_memory())  # physical memory usage
    pid = os.getpid()
    py = psutil.Process(pid)
    memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think
    print('memory GB:', memoryUse)


class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        # x = torch.sigmoid(self.map3(x))

        # return torch.max(x_and_example[0], (x*x_and_example[2]))
        return torch.max(x_and_example[0], x)


class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.sigmoid(self.map1(x))
        return torch.sigmoid(self.map2(x))


class GAN():
    def __init__(self, input_size, hidden_size, noise_size):
        self.otiginal_data_size = 0
        self.noise_size = noise_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.criterion_ = nn.L1Loss()
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=1).cuda()
        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.001, betas=(0.9, 0.999))
        self.G = Generator(input_size=input_size + self.noise_size, hidden_size=hidden_size,
                           output_size=input_size).cuda()
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.0001, betas=(0.9, 0.999))

    def plot_added_featues(self,list_of_added_features, round):
        frams = pd.DataFrame(list_of_added_features)
        frams.to_csv('./male_qabl/list_of_added_features'+str(round)+'.csv')
        ax = frams.plot.bar(stacked=True)
        ax.set_xlabel('EPOCH')
        # for rowNum, row in frams.iterrows():
        #     ypos = 0
        #     featuer=0
        #     for val in row:
        #         if featuer!=6:
        #             ypos += val
        #             ax.text(rowNum, ypos , "{0:.2f}".format(val), color='black' ,ha='center')
        #         featuer+=1
        #     ypos = 0
        plt.title('Average Number of Added Features in Each Epoch')
        plt.savefig('./male_qabl/added_featues({0}).png'.format(round))

        # chartBox = ax.get_position()
        # ax.set_position([chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])
        # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.8), ncol=1)
        # plt.show()
        plt.close()

    def process_batch(self , local_batch , local_lable):

        xmal_batch = local_batch[(local_lable != 0).nonzero()]
        # print(xmal_batch.shape)

        if (len(xmal_batch.shape) > 2):
            xmal_batch = xmal_batch.reshape((xmal_batch.shape[0] * xmal_batch.shape[1]), xmal_batch.shape[2])
            # print(xmal_batch.shape)

        xben_batch = local_batch[(local_lable == 0).nonzero()]
        # print(xben_batch.shape)
        if len(xben_batch.shape) > 2:
            xben_batch = xben_batch.reshape((xben_batch.shape[0] * xben_batch.shape[1]), xben_batch.shape[2])
        # print(xben_batch.shape)

        return xmal_batch , xben_batch

    def check_added_features(self, x, features):
        added_features = []
        for i in range(len(x)):
            unique, counts = np.unique(
                np.where(x.cpu().numpy()[i].astype(int) > 0, features[:, 2], np.zeros(features[:, 2].shape)),
                return_counts=True)
            unique = unique[1:]
            counts = counts[1:]
            added_features.append(dict(zip(unique, counts)))

        batch_added_features = dict(functools.reduce(operator.add, map(collections.Counter, added_features)))
        batch_added_features.update({n: batch_added_features[n] / len(x) for n in batch_added_features.keys()})
        return batch_added_features

    def train(self, _classifier, epochs, data, batch_size=32, round=-1):

        early_stopping = EarlyStopping(patience=20, verbose=True)

        # Load and Split the dataset

        # (x_org_mal, y_org_mal)= data[0][0][:data_size[0]], data[0][1][:data_size[0]]
        # (x_org_ben, y_org_ben) = data[1][0][:data_size[0]], data[1][1][:data_size[0]],
        # (xmal, ymal), (xben, yben) =  data[0], data[1]
        #
        # X_org = np.concatenate([x_org_mal, x_org_ben])
        # Y_org = np.concatenate([y_org_mal, y_org_ben])
        #
        # X_org_adv = np.concatenate([xben, xmal])
        # Y_org_adv = np.concatenate([yben, ymal])
        # xtest, ytest =  data[2][0], data[2][1]

        # create manifest feature mask
        features = np.load('./MalwareDataset/Drebin_important_features.npy')
        features=np.append(features,[features[len(features)-1]], axis=0)
        # manifest_features =np.where((features[:, 2] == 'activity') | (features[:, 2] == 'feature') | (features[:, 2] == 'permission') | (
        #             features[:, 2] == 'service_receiver') | (features[:, 2] == 'provider') | (
        #                      features[:, 2] == 'service') | (features[:, 2] == 'intent'), np.ones(len(features)),
        #          np.zeros(len(features)))
        # manifest_features = torch.from_numpy(manifest_features).float().cuda()



        (xmal, ymal), (xben, yben) = data[0], data[1]
        X = np.concatenate([xben, xmal])
        Y = np.concatenate([yben, ymal])
        xtrain, ytrain, xtest, ytest = X, Y, data[2][0], data[2][1]

        # # sampling for unbalanced data
        # class_sample_count = np.array(
        #     [len(np.where(ytrain == t)[0]) for t in np.unique(ytrain)])
        # weight = 1. / class_sample_count
        # samples_weight = []
        # for t in range(len(ytrain) - 1):
        #     samples_weight.append(weight[int(ytrain[t])])
        # sampler = data_utils.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))
        train = data_utils.TensorDataset(torch.from_numpy(xtrain), torch.from_numpy(ytrain))
        num_train = len(train)
        indices = list(range(num_train))
        np.random.shuffle(indices)
        split = int(np.floor(0.2 * num_train))
        train_idx, valid_idx = indices[split:], indices[:split]
        train_sampler = data_utils.sampler.WeightedRandomSampler(train_idx, len(train_idx))
        valid_sampler = data_utils.sampler.WeightedRandomSampler(valid_idx, len(valid_idx))

        # train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=sampler)
        train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=train_sampler)
        valid_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=valid_sampler)

        print ('\nTRAINING GAN...\n')
        start_train = timer()
        list_of_added_features = []

        Train_FNR, Test_FNR = [], []
        best_test_FNR, best_train_FNR = 0.0, 0.0
        self.gtrainloss, self.gvalidloss, self.dloss = [], [],[]
        self.avg_train_losses = []
        self.avg_valid_losses = []
        for epoch in range(epochs):

            batch_added_features=[]
            self.G.train()
            self.D.train()

            for local_batch, local_lable in train_loader:
                # ---------------------
                #  Train substitute_detector
                # ---------------------

                xmal_batch ,xben_batch = self.process_batch(local_batch , local_lable)
                if xmal_batch.shape[0]<1 or xben_batch.shape[0]<1:
                    break
                else:


                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                    yclassifierben_batch = _classifier.predict(xben_batch)

                    # Generate a batch of new malware examples
                    gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()#,manifest_features]).detach()
                    batch_added_features.append(
                        self.check_added_features((torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()) - xmal_batch.cuda().float(), features))
                    yganmal_batch = _classifier.predict(
                        np.ones(gen_examples.cpu().detach().numpy().shape) * (gen_examples.cpu().detach().numpy() > 0.5))

                    # Train the substitute_detector
                    d_fake_decision = self.D(torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                    d_loss_fake = self.criterion(d_fake_decision.squeeze(), torch.from_numpy(yganmal_batch).float().cuda())
                    d_real_decision = self.D(xben_batch.float().cuda())
                    d_loss_real = self.criterion(d_real_decision.squeeze(), torch.from_numpy(yclassifierben_batch).float().cuda())
                    d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)

                    self.D.zero_grad()
                    d_loss.backward(retain_graph=True)
                    self.d_optimizer.step()

                    # ---------------------
                    #  Train Generator
                    # ---------------------

                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)

                    # Train the generator
                    g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()#,manifest_features]).detach()
                    dg_fake_decision = self.D(g_fake_data)
                    g_loss_samples = self.criterion(dg_fake_decision.squeeze(), torch.zeros(xmal_batch.shape[0]).cuda())
                    # orig_adv_dist = np.diag(
                    #     sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                    #                                                  xmal_batch))
                    # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                    # g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                    #                                   torch.zeros(orig_adv_dist.shape[0]).cuda())
                    g_loss = g_loss_samples
                    self.G.zero_grad()
                    g_loss.backward()
                    self.g_optimizer.step()
                    self.gtrainloss.append(g_loss)

                    torch.cuda.empty_cache()

            for data, target in valid_loader:
                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                g_fake_data = self.G(
                    [xmal_batch.float().cuda(), noise.float().cuda()]).detach()  # ,manifest_features]).detach()
                dg_fake_decision = self.D(g_fake_data)
                g_loss_samples = self.criterion(dg_fake_decision.squeeze(), torch.zeros(xmal_batch.shape[0]).cuda())
                # calculate the loss
                # record validation loss
                self.gvalidloss.append(g_loss_samples)
            self.avg_train_losses.append(torch.mean(torch.stack(self.gtrainloss)))
            self.avg_valid_losses.append(torch.mean(torch.stack(self.gvalidloss)))
            self.gtrainloss=[]
            self.gvalidloss=[]
            self.dloss.append(d_loss)

            early_stopping(np.average(self.gvalidloss), self.G)
            if early_stopping.early_stop:
                print("Early stopping")
                break

            list_of_added_features.append(pd.DataFrame(batch_added_features).mean(axis=0).to_dict())
            # Compute attach success rate on train data
            xtrain_mal = xtrain[(ytrain == 1).nonzero()[0]]
            noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtrain_mal).float().cuda(), torch.from_numpy(noise).float().cuda()]).detach()#,manifest_features]).detach()
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)
            if (sum(np.ones(gen_examples.shape[0])- classifier_.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                train_FNR = 0

            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            Train_FNR.append(train_FNR)
            # Compute attach success rate on test data
            xtest_mal = xtest[(ytest != 0).nonzero()[0]]
            noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtest_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])#,manifest_features])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)

            if (sum(np.ones(gen_examples.shape[0])- classifier_.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                test_FNR = 0
            Test_FNR.append(test_FNR)
            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            if test_FNR > best_test_FNR:
                best_test_FNR = test_FNR
                print('saving mulgan weights at epoch:', epoch)

                print("[D loss: %f] [G loss: %f] \n" % (self.dloss[-1], self.gloss[-1]))
                torch.save(self.G,
                           './male_qabl/malgan{0}.pt'.format(round))

            # Print the progress
            # if epoch % 20 == 0:
            # print(pd.DataFrame(batch_added_features).mean(axis=0))
            print("%d [D loss: %f] [G loss: %f] [test_FNR: %f] [train_FNR: %f] [disstortion: %f]" % (epoch, d_loss, g_loss , test_FNR,train_FNR , pd.DataFrame(batch_added_features).mean(axis=0).sum()))
        end_train = timer()
        del g_loss, d_loss

        print('\ntraining completed in %.3f seconds.\n' % (end_train - start_train))
        print('=============results ============= ')

        print(' attack success rate using train data: {0} \n'
              ' attack success rate using test data: {1}'.format(best_train_FNR, best_test_FNR))
        print('==============================\n ')

        self.plot_added_featues(list_of_added_features , round)

        # Plot losses
        plt.figure()
        plt.plot(range(len(self.gloss)), self.gloss, c='r', label='g_loss_rec', linewidth=2)
        plt.plot(range(len(self.dloss)), self.dloss, c='g', linestyle='--', label='d_loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('loss')
        plt.legend()
        plt.savefig('./male_qabl/GAN_Epoch_loss({0}).png'.format(round))
        # plt.show()
        plt.close()

        # Plot TPR
        plt.figure()
        plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Training Set', linewidth=2)
        plt.plot(range(len(Test_FNR)), Test_FNR, c='g', linestyle='--', label='Validation Set', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('FNR')
        plt.legend()
        plt.savefig('./male_qabl/Epoch_FNR({0}).png'.format(round))
        # plt.show()
        plt.close()



        return [best_train_FNR, best_test_FNR]


def build_classifier(type='RF', data=[]):
    if type is 'RF':
        model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=1)
    elif type is 'SVM':
        model = svm.SVC()
    elif type is 'RBF_SVM':
        # Parameters={'kernel':['rbf'],'gamma': [1e-3, 1e-4],'C': [0.001, 0.01, 0.1]}
        # model = GridSearchCV(svm.SVC(), Parameters, cv=5, scoring='f1', n_jobs=-1)
        model = svm.SVC(kernel='rbf')
    elif type is 'LR':
        model = linear_model.LogisticRegression()
    elif type is 'DT':
        model = tree.DecisionTreeRegressor()
    elif type is 'MLP':
        model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                              solver='sgd', verbose=0, tol=1e-4, random_state=1,
                              learning_rate_init=.1)

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data[0], data[1], data[2], data[3]
    # xtrain = scipy.sparse.vstack((xben, xmal))
    # ytrain = np.concatenate([yben, ymal])
    # xtest = scipy.sparse.vstack((xtsmal, xtsben))
    # ytest = np.concatenate([ytsmal, ytsben])
    xtrain = np.concatenate([xben, xmal])
    ytrain = np.concatenate([yben, ymal])
    xtest = np.concatenate([xtsmal, xtsben])
    ytest = np.concatenate([ytsmal, ytsben])

    print('\n\n--- Round: {0} '.format(0))

    print('=============data============= ')
    print(
        'number of original malware = {0}\n'
        'number of adversarial samples (old+new) = {1}\n'
        'number of new adversarial samples = {2}\n'
        'Malware to benign Ratio: {3:.2f}\n'
        'adversarial samples portion of malware samples = {4:.2f}\n'
        'adversarial samples portion of all samples = {5:.2f} '
            # .format(len(xmal) + len(data[2][0][(data[2][1]==1).nonzero()[0]]),
            .format(xmal.shape[0],
                    0,
                    0,
                    xmal.shape[0] / xben.shape[0],
                    0,
                    0
                    ))
    print('==============================\n ')


    print ('TRAINING CLASSIFIER...\n')
    start_train = timer()
    if os.path.isfile('./male_qabl/' + type + '__' + str(por) + '.sav'):
        # if os.path.isfile('/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav'):
        model = pickle.load(open('./male_qabl/' + type + '__' + str(por) + '.sav', 'rb'))
        # '/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav', 'rb'))
    else:
        model.fit(xtrain, ytrain)
        pickle.dump(model, open('./male_qabl/' + type + '__' + str(por) + '.sav', 'wb'))
    end_train = timer()

    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('=============results ============= ')

    conf_matx = confusion_matrix(ytrain, model.predict(xtrain))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1], conf_matx[1][0],
                                                           conf_matx[1][1]))
    train_FNR =  conf_matx[1][0]/(conf_matx[1][0]+conf_matx[1][1])

    conf_matx = confusion_matrix(ytest, model.predict(xtest))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1], conf_matx[1][0],
                                                           conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Original_Train_TPR = model.score(xtrain, ytrain)
    Original_Test_TPR = model.score(xtest, ytest)
    print('Original_Train_TPR: {0}, Original_Test_TPR: {1} , FNR for train: {2} , FNR for test: {3}'. format(Original_Train_TPR , Original_Test_TPR , train_FNR ,test_FNR ) )
    print('==============================\n ')
    fpr, tpr, _ = roc_curve(ytest, model.predict(xtest))
    roc_auc = auc(fpr, tpr)
    accuracy= accuracy_score(ytest, model.predict(xtest))
    f1= f1_score(ytest, model.predict(xtest))
    precision= precision_score(ytest, model.predict(xtest))
    return model , [Original_Train_TPR , Original_Test_TPR , Original_Test_TPR , train_FNR ,test_FNR, test_FNR ,accuracy,f1,precision],[ fpr, tpr, roc_auc ]


def retrain_classifier(classifier, _GAN, data, round):
    # create manifest feature mask
    features = np.load(
        '/home/maryam/Code/data/Drebin_processed_features/randomforest_important_features/0.001/Drebin_important_features.npy')
    features = np.append(features, [features[len(features) - 1]], axis=0)
    manifest_features = np.where(
        (features[:, 2] == 'activity') | (features[:, 2] == 'feature') | (features[:, 2] == 'permission') | (
                features[:, 2] == 'service_receiver') | (features[:, 2] == 'provider') | (
                features[:, 2] == 'service') | (features[:, 2] == 'intent'), np.ones(len(features)),
        np.zeros(len(features)))
    manifest_features = torch.from_numpy(manifest_features).float().cuda()


    print('\n\n--- Round: {0} '.format(round+1))

    print('\n\nADVESARIAL RETRAINING...\n\n')

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben)= data[0], data[1], data[2], data[3]
    xtest= np.concatenate([xtsmal,xtsben])
    ytest= np.concatenate([ytsmal,ytsben])
    xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # Generate Train Adversarial Examples
    print('Generating Train Adversarial Examples...\n')
    noise = np.random.uniform(0, 1, (data_size[0], _GAN.noise_size))
    _GAN.G = torch.load('./male_qabl/malgan{0}.pt'.format(round))
    with torch.no_grad():
        # not to generate adv version from other avd samples
        gen_examples = _GAN.G([torch.from_numpy(data[0][0][:data_size[0]]).float().cuda(),
                               torch.from_numpy(noise).float().cuda(),manifest_features]).detach()
    x_gen_examples = (torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()).cpu().numpy()
    x_gen_examples = np.unique(x_gen_examples, axis=0)
    y_gen_examples = np.ones(len(x_gen_examples))
    train_gen_examples, test_gen_examples, y_train_gen_examples, y_test_gen_examples = train_test_split(x_gen_examples,
                                                                                                        y_gen_examples,
                                                                                                        test_size=0.20)
    data_size.append(len(x_gen_examples))
    print('==============new data============ ')
    print(
        'number of original malware = {0}\n'
        'number of adversarial samples (old+new) = {1}\n'
        'number of new adversarial samples = {2}\n'
        'Malware to benign Ratio: {3:.2f}\n'
        'adversarial samples portion of malware samples = {4:.2f}\n'
        'adversarial samples portion of all samples = {5:.2f} '
            .format(data_size[0],
                    np.sum(data_size) - data_size[0],
                    len(x_gen_examples),
                    len(data[0][0]) / len(data[1][0]),
                    (np.sum(data_size) - data_size[0]) / np.sum(data_size),
                    (np.sum(data_size) - data_size[0]) / (np.sum(data_size) + len(data[1][0]))
                    ))
    print('==================================\n ')
    # #select the closest adv samples to original samples to add to training set
    # orig_adv_dist = np.diag(sklearn.metrics.pairwise.euclidean_distances(gen_examples, data[0][0][:data_size[0]]))
    # orig_adv_dist_train = np.diag(sklearn.metrics.pairwise.manhattan_distances(gen_examples, data[0][0][:data_size[0]]))
    # # select best 20 percent of adversarial samples to add to training set and teach the classifier
    # # portion_to_add_to_train_data = np.int(0.2 * len(orig_adv_dist))
    # portion_to_add_to_train_data = np.int(1.0 * len(orig_adv_dist))
    # indext_of_best_adv_samples = np.argpartition(orig_adv_dist, portion_to_add_to_train_data)[:portion_to_add_to_train_data]
    # adv_to_add_to_train = gen_examples[indext_of_best_adv_samples]

    print('retraining classifier...\n')
    start_train = timer()

    classifier.fit(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                   np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    end_train = timer()
    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('\n=============results ============= ')

    # Compute Train TPR
    train_TPR = classifier.score(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples])),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1],
                                                                         conf_matx[1][0],
                                                                         conf_matx[1][1]))
    train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    # Compute Test
    test_TPR = classifier.score(np.concatenate([xtest_mal, xtest_ben, test_gen_examples]),
                                np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))

    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtest_mal, xtest_ben, test_gen_examples])),
                                 np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Clean_test_TPR = classifier.score(xtest, ytest)

    conf_matx = confusion_matrix(classifier.predict(xtest), ytest)
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    Clean_test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    print(
        'Train accuracy: {0}\n Test accuracy: {1}  \n FNR for train: {2} \n FNR for test: {3} \n Clean_test_TPR: {4} \n Clean_test_FNR: {5}  '
            .format(train_TPR, test_TPR, train_FNR, test_FNR, Clean_test_TPR, Clean_test_FNR))
    print('==============================\n ')
    fpr, tpr, _ = roc_curve(ytest, classifier.predict(xtest))
    roc_auc = auc(fpr, tpr)
    accuracy = accuracy_score(ytest, classifier.predict(xtest))
    f1 = f1_score(ytest, classifier.predict(xtest))
    precision = precision_score(ytest, classifier.predict(xtest))
    # keeping the generated samples from previous rounds
    data[0] = (np.concatenate([data[0][0], x_gen_examples]), np.concatenate([data[0][1], np.ones(len(x_gen_examples))]))

    # plot adv and orig samples distance
    orig_adv_dist = np.sum(x_gen_examples-xmal[:data_size[0]],1)


    plot_dist.append(orig_adv_dist[range(len(x_gen_examples) - 1)])

    plt.figure()
    plt.boxplot(plot_dist, labels=[i for i in range(len(plot_dist))])

    # plt.show()

    plt.xlabel('round of retraining')#(54 adv. sample in each round is added to dataset)')
    plt.ylabel('Generated samples disstortion (L1 norm)')
    plt.savefig('./male_qabl/new_adv_samples_distortion.png')

    # plt.legend()
    # plt.show()
    # print(orig_adv_dist)

    return [train_TPR , test_TPR , Clean_test_TPR , train_FNR ,test_FNR, Clean_test_FNR  ,accuracy,f1,precision],[ fpr, tpr, roc_auc]


def load_data():
    xtrain = scipy.sparse.load_npz('./MalwareDataset/x_train.npz').toarray()
    ytrain = np.load('./MalwareDataset/y_train.npy')
    xtrain_mal = xtrain[np.where(ytrain == 1)]
    xtrain_ben = xtrain[np.where(ytrain == 0)]
    ytrain_mal = ytrain[np.where(ytrain == 1)]
    ytrain_ben = ytrain[np.where(ytrain == 0)]
    # xtrain_mal = xtrain_mal[0:1000]
    # xtrain_ben = xtrain_ben[0:1000]
    # ytrain_mal =ytrain_mal[0:1000]
    # ytrain_ben = ytrain_ben[0:1000]
    xtest = scipy.sparse.load_npz('./MalwareDataset/x_test.npz').toarray()
    ytest = np.load('./MalwareDataset/y_test.npy')
    features = np.load('./MalwareDataset/Drebin_important_features.npy')
    features = np.append(features, [features[len(features) - 1]], axis=0)

    xtest_mal = xtest[np.where(ytest == 1)]
    xtest_ben = xtest[np.where(ytest == 0)]
    ytest_mal = ytest[np.where(ytest == 1)]
    ytest_ben = ytest[np.where(ytest == 0)]

    # xtest_mal = xtest_mal[0:500]
    # xtest_ben =  xtest_ben[0:500]
    # ytest_mal = ytest_mal[0:500]
    # ytest_ben =   ytest_ben[0:500]
    # # data = np.load('/home/maryam/Code/python/Malware-GAN-master/data.npz')
    # xben = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_benign_binary_features.npy')
    # xmal = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_malware_binary_features.npy')
    # # xmal = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_malware_binary_features.npy')
    # # xben = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_benign_binary_features.npy')
    #
    # # xmal = xmal[0:200]
    # ymal = np.ones(len(xmal))
    #
    # xben = xben[0:len(xmal)]
    # # xben = xben[0:len(xmal)+1000]
    # yben = np.zeros(len(xben))
    # # xmal, ymal, xben, yben = data['xmal'], data['ymal'], data['xben'], data['yben']
    # # ymal, yben = np.ones(len(xmal)) , np.zeros(len(xben))
    # # same number of malware and bengin samples
    # # return [(xmal[0,200], ymal[0,200]), (xben[0,200], yben[0,200])]
    # # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # # return [(xmal, ymal), (xben[0:len(xmal)], yben[0:len(xmal)])]
    # print(
    #     'number of malware for training = {0}\n'
    #     'number of benign for training= {1}\n'
    #     'number of malware for test  = {2}\n'
    #     'number of benign for test= {3}\n'
    #         .format(len(xtrain_mal),
    #                 len(xtrain_ben),
    #                 len(xtest_mal),
    #                 len(xtest_ben)))
    return [(xtrain_mal, ytrain_mal), (xtrain_ben, ytrain_ben),
            (xtest_mal, xtest_ben), (ytest_mal, ytest_ben)]
    # return [(xmal[0:round(len(xmal)/5)], ymal[0:round(len(ymal)/5)]), (xben, yben)]

def plot_roc(roc_curve_values):
    plt.figure()
    for i in range(len(roc_curve_values)):
        plt.plot(roc_curve_values[i][0], roc_curve_values[i][1],
             lw=2, label='Round %d (auc = %0.4f)' % (i,roc_curve_values[i][2]))
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.savefig('./male_qabl/roc_all.eps'.format(round))

    plt.show()

    plt.close()

    plt.figure()
    plt.plot(roc_curve_values[0][0], roc_curve_values[0][1],
                 lw=2, label='Original (auc = %0.4f)' % roc_curve_values[0][2])
    plt.plot(roc_curve_values[-1][0], roc_curve_values[-1][1],
                 lw=2, label='Round 10 (auc = %0.4f)' % roc_curve_values[-1][2])
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.savefig('./male_qabl/roc_first_last.eps'.format(round))

    plt.show()

    plt.close()


if __name__ == '__main__':


    # frams = pd.read_csv('./models/list_of_added_features.csv', encoding='utf-8')
    # ax = frams.plot.bar(stacked=True)
    # ax.set_xlabel('EPOCH')
    # plt.xticks(np.arange(0,50,  step=10))

    classifiers = ['SVM','RF' , 'RBF_SVM', 'LR', 'MLP']

    for type in classifiers:
        print('\n====================================')
        print('EXPERIMENT ON <<' + type + '>> CLASSIFIER:')
        results = []
        data_all = load_data()
        # data_all= load_data.load_data(loaded= True)
        mal_num = data_all[0][0].shape[0]
        ben_num = data_all[1][0].shape[0]
        hist = {}
        hist['0.1'] = []
        hist['0.2'] = []
        hist['0.3'] = []
        hist['0.4'] = []
        hist['0.5'] = []
        por= 0.5 #, 0.4 , 0.3 , 0.2 ,0.1  ]:
        if por>0:
            porr = int(mal_num / por - mal_num)
        else:
            porr = ben_num-1
        print(
            'number of malware for training = {0}\n'
            'number of benign for training= {1}\n'
            'Malware portion  = {2}\n'
                .format(mal_num,
                        porr, por ))

        data_original = [#(xmal, ymal)
                data_all[0] ,
                #(xben,yben )
                (data_all[1][0][0:porr] ,data_all[1][1][0:porr]),
                #(xtsmal, xtsben)
                (data_all[2][0],data_all[2][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ,#portion of train to test
                # (ytsmal, ytsben)
                (data_all[3][0] , data_all[3][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ]#portion of train to test
        data = [#(xmal, ymal)
                    data_all[0] ,
                    #(xben,yben )
                    (data_all[1][0][0:porr] ,data_all[1][1][0:porr]),
                    #(xtsmal, xtsben)
                    (data_all[2][0],data_all[2][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ,#portion of train to test
                    # (ytsmal, ytsben)
                    (data_all[3][0] , data_all[3][1][0:int((data_all[1][0][0:porr] .shape[0])*(0.3/0.7))]) ]#portion of train to test



        plot_dist = []
        GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=500, noise_size=50)
        data_size = [len(data_original[0][0])]
        classifier_ , result_cls,roc= build_classifier(type=type, data=data_original)
        roc_curve_values= [roc]
        result_gan = GAN_.train(classifier_, epochs=100, data=data_original, batch_size=100, round=0)
        results.append(np.concatenate([[0] , result_cls , result_gan]))
        # for round in range(10):
        #     data[1] = (data_all[1][0][0:porr+data_size[0]], data_all[1][1][0:porr+data_size[0]])
        #     result_cls ,roc= retrain_classifier(classifier_, GAN_, data, round=round)
        #     del GAN_
        #     GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=20)
        #     result_gan = GAN_.train(classifier_, epochs=50, data=data_original, batch_size=100, round=round+1)
        #     results.append(np.concatenate([[round+1] ,result_cls, result_gan]))
        #     roc_curve_values.append(roc)
        #
        #
        # plot_roc(roc_curve_values)
        # Header = ['Round', 'Train_accuray', 'Test_accuray', 'Test_validationset_accuray' , 'Train_FNR', 'Test_FNR', 'Test_validationset_FNR'  ,'accuracy','f1','precision', 'MCR on train data' , 'MCR on test data' ]
        # print("Writing data ...")
        # with open('./male_qabl/results.csv', 'w', newline='') as outfile1:
        #     wr = csv.writer(outfile1, delimiter=',', quoting=csv.QUOTE_NONE)
        #     wr.writerow([h for h in Header])
        #     for i in range(len(results)):
        #         wr.writerow(results[i])
        #     outfile1.close()
