import itertools

import numpy as np
from timeit import default_timer as timer

import sklearn
import scipy.sparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt



class Generator_using_malware_and_noise(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_malware_and_noise, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)
class Generator_using_noise_only(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator_using_noise_only, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        # self.map2 = nn.Linear(hidden_size, hidden_size)
        # self.map3 = nn.Linear(hidden_size, output_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        #x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x_and_example[1]))
        x = torch.sigmoid(self.map2(x))
        # x = F.sigmoid(self.map3(x))

        return torch.max(x_and_example[0], x)

class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.sigmoid(self.map1(x))
        return torch.sigmoid(self.map2(x))

def spy_sparse2torch_sparse(data):
    """

    :param data: a scipy sparse csr matrix
    :return: a sparse torch tensor
    """
    samples=data.shape[0]
    features=data.shape[1]
    values=data.data
    coo_data=data.tocoo()
    indices=torch.LongTensor([coo_data.row,coo_data.col])
    t=torch.sparse.FloatTensor(indices,torch.from_numpy(values).float(),[samples,features]).to_dense()
    return t

class GAN():
    def __init__(self, input_size, hidden_size, noise_size , generator_type , losstype):
        self.otiginal_data_size = 0
        self.noise_size = noise_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.criterion_ = nn.L1Loss()
        self.losstype = losstype
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=1).cuda()
        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.0001, betas=(0.9, 0.999))
        if generator_type == 'Mal+Noise':
            self.G = Generator_using_malware_and_noise(input_size=input_size + self.noise_size, hidden_size=hidden_size,
                           output_size=input_size).cuda()
        if generator_type == 'Noise':
            self.G = Generator_using_noise_only(input_size=self.noise_size, hidden_size=hidden_size,
                                                       output_size=input_size).cuda()
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.0001, betas=(0.9, 0.999))

    def train(self, classifier, epochs, data_original, batch_size=32, round=-1 , por=0.1):
        # Load and Split the dataset

        # (x_org_mal, y_org_mal)= data[0][0][:data_size[0]], data[0][1][:data_size[0]]
        # (x_org_ben, y_org_ben) = data[1][0][:data_size[0]], data[1][1][:data_size[0]],
        # (xmal, ymal), (xben, yben) =  data[0], data[1]
        #
        # X_org = np.concatenate([x_org_mal, x_org_ben])
        # Y_org = np.concatenate([y_org_mal, y_org_ben])
        #
        # X_org_adv = np.concatenate([xben, xmal])
        # Y_org_adv = np.concatenate([yben, ymal])
        # xtest, ytest =  data[2][0], data[2][1]
        (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data_original[0], data_original[1], data_original[2], data_original[3]
        xtrain = scipy.sparse.vstack((xben, xmal))
        ytrain = np.concatenate([yben, ymal])
        xtest = scipy.sparse.vstack((xtsmal, xtsben))
        ytest = np.concatenate([ytsmal, ytsben])

        xtrain=spy_sparse2torch_sparse(xtrain)



        # xtrain = np.concatenate([xben, xmal])
        # ytrain = np.concatenate([yben, ymal])
        # xtest = np.concatenate([xtsmal, xtsben])
        # ytest = np.concatenate([ytsmal, ytsben])

        # xtrain=torch.from_numpy(xtrain.toarray())
        #sampling for unbalanced data
        class_sample_count = np.array(
            [len(np.where(ytrain == t)[0]) for t in np.unique(ytrain)])
        weight = 1. / class_sample_count
        samples_weight = []
        for t in range(len(ytrain) - 1):
            samples_weight.append(weight[int(ytrain[t])])
        sampler = data_utils.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))
        train = data_utils.TensorDataset(spy_sparse2torch_sparse(xtrain), torch.from_numpy(ytrain))
        # train = data_utils.TensorDataset(torch.from_numpy(xtrain), torch.from_numpy(ytrain))
        train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=sampler , drop_last = True)

        print ('\nTRAINING GAN...\n')
        print('=============data============= ')
        print(
            'number of original malware = {0}\n'
            'number of original ben = {1}\n'
            'number of original test = {2}\n'
                # .format(len(xmal) + len(data[2][0][(data[2][1]==1).nonzero()[0]]),
                .format(xmal.shape[0], xben.shape[0], xtsmal.shape[0]))
        print('==============================\n ')
        start_train = timer()
        list_of_distortion=[]
        Train_FNR, Test_FNR = [], []
        best_test_FNR, best_train_FNR = 0.0, 0.0
        self.gloss, self.dloss = [], []
        for epoch in range(epochs):
            for local_batch, local_lable in train_loader:

                # ---------------------
                # ---------------------
                #  Train discriminator
                # ---------------------
                # ---------------------
                # xmal_batch = local_batch[(local_lable != 0).nonzero()]
                # if (len(xmal_batch.shape) > 2):
                #     xmal_batch = xmal_batch.reshape((xmal_batch.shape[0] * xmal_batch.shape[1]), xmal_batch.shape[2])
                # xben_batch = local_batch[(local_lable == 0).nonzero()]
                # if len(xben_batch.shape) > 2:
                #     xben_batch = xben_batch.reshape((xben_batch.shape[0] * xben_batch.shape[1]), xben_batch.shape[2])
                # noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                # #yclassifierben_batch = classifier.predict(xben_batch)
                #
                # # Generate a batch of new malware examples
                # gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()
                # #yganmal_batch = classifier.predict(
                #    # np.ones(gen_examples.cpu().detach().numpy().shape) * (gen_examples.cpu().detach().numpy() > 0.5))
                #
                # # Train the substitute_detector
                # d_fake_decision = self.D(torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                # d_loss_fake = self.criterion(d_fake_decision, torch.zeros(gen_examples.shape[0]).cuda())
                # d_real_decision = self.D(xben_batch.float().cuda())
                # d_loss_real = self.criterion(d_real_decision, torch.ones(xben_batch.shape[0]).cuda())
                # d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)
                #
                # self.D.zero_grad()
                # d_loss.backward(retain_graph=True)
                # self.d_optimizer.step()
                #
                # # ---------------------
                # #  Train Generator
                # # ---------------------
                #
                # noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                #
                # # Train the generator
                # g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()])
                # dg_fake_decision = self.D(g_fake_data)
                # #try having non zero value as the good result we want
                # g_loss_samples = self.criterion(dg_fake_decision, torch.zeros(xmal_batch.shape[0]).cuda())
                # if self.losstype == 'limited_distortion':
                #     orig_adv_dist = np.diag(
                #         sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                #                                                      xmal_batch))
                #     # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                #     g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                #                                       torch.zeros(orig_adv_dist.shape[0]).cuda())
                #
                #     g_loss = 0.99 * g_loss_samples + 0.01 * g_loss_distance
                # if self.losstype == 'unlimited_distorion':
                #     g_loss = g_loss_samples
                #
                # self.G.zero_grad()
                # g_loss.backward()
                # self.g_optimizer.step()

                # ---------------------------------------------------------------
                # ---------------------------------------------------------------



                # ---------------------
                #  Train substitute_detector
                # ---------------------

                xmal_batch = local_batch[(local_lable != 0).nonzero()]
                if (len(xmal_batch.shape) > 2):
                    xmal_batch = xmal_batch.reshape((xmal_batch.shape[0] * xmal_batch.shape[1]), xmal_batch.shape[2])
                xben_batch = local_batch[(local_lable == 0).nonzero()]
                if len(xben_batch.shape) > 2:
                    xben_batch = xben_batch.reshape((xben_batch.shape[0] * xben_batch.shape[1]), xben_batch.shape[2])
                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                yclassifierben_batch = classifier.predict(xben_batch)

                # Generate a batch of new malware examples
                gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()
                yganmal_batch = classifier.predict(
                    np.ones(gen_examples.cpu().detach().numpy().shape) * (gen_examples.cpu().detach().numpy() > 0.5))

                # # Train the substitute_detector
                # d_fake_decision = self.D(torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                # d_loss_fake = self.criterion(d_fake_decision, torch.from_numpy(yganmal_batch).float().cuda().view(-1,1))
                # d_real_decision = self.D(xben_batch.float().cuda())
                # d_loss_real = self.criterion(d_real_decision, torch.from_numpy(yclassifierben_batch).float().cuda().view(-1,1))
                # d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)

                # Train a real discriminator
                d_fake_decision = self.D(torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                d_loss_fake = self.criterion(d_fake_decision,
                                             torch.ones(d_fake_decision.shape).cuda())
                d_real_decision = self.D(xben_batch.float().cuda())
                d_loss_real = self.criterion(d_real_decision,
                                             torch.zeros(d_real_decision.shape).cuda())
                d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)



                self.D.zero_grad()
                d_loss.backward(retain_graph=True)
                self.d_optimizer.step()

                # ---------------------
                #  Train Generator
                # ---------------------

                noise = torch.rand(xmal_batch.shape[0], self.noise_size)

                # Train the generator
                g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()])
                dg_fake_decision = self.D(g_fake_data)
                g_loss_samples = self.criterion(dg_fake_decision, torch.zeros(dg_fake_decision.shape).cuda())
                if self.losstype == 'limited_distortion':
                    orig_adv_dist = np.diag(
                        sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                                                                     xmal_batch))
                    # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                    g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                                                      torch.zeros(orig_adv_dist.shape[0]).cuda().view(-1,1))
                    # g_loss = 0.7 * g_loss_samples + 0.3 * g_loss_distance
                    g_loss = g_loss_samples
                    list_of_distortion.append(orig_adv_dist)
                if self.losstype == 'unlimited_distorion':
                    g_loss = g_loss_samples
                    #
                    # orig_adv_dist = np.diag(
                    #     sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                    #                                                  xmal_batch))
                    # # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                    # g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                    #                                   torch.zeros(orig_adv_dist.shape[0]).cuda().view(-1, 1))
                    # print('diiiiiiiiiiiiiiiiiiiiiiid', g_loss_distance)


                self.G.zero_grad()
                g_loss.backward()
                self.g_optimizer.step()

                torch.cuda.empty_cache()

            self.gloss.append(g_loss)
            self.dloss.append(d_loss)
            orig_adv_dist = np.diag(
                    sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                                                                 xmal_batch))
            # Compute attach success rate on train data
            # xtrain_mal = xtrain[(ytrain == 1).nonzero()[0]]
            xtrain_mal = xmal
            noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtrain_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)
            if (sum(np.ones(gen_examples.shape[0])- classifier.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier.predict(gen_examples))
                train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                train_FNR = 0

            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
            Train_FNR.append(train_FNR)


            # Compute attack success rate on test data
            # xtest_mal = xtest[(ytest != 0).nonzero()[0]]
            xtest_mal = xtsmal
            noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtest_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) *(gen_examples.cpu().detach().numpy() > 0.5)

            if (sum(np.ones(gen_examples.shape[0])- classifier.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier.predict(gen_examples))
                test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                test_FNR = 0
            Test_FNR.append(test_FNR)
            if test_FNR > best_test_FNR:
                best_test_FNR = test_FNR
                print('saving mulgan weights at epoch: %d', epoch)

                print("[G loss: %f] [D loss: %f] \n" % (self.gloss[-1], self.dloss[-1]))
                torch.save(self.G,
                           '/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/malgan{0}.pt'.format(round))

            # Print the progress
            # if epoch % 20 == 0:
            print("%d [D loss: %f] [G loss: %f] [train_FNR: %f] [test_FNR: %f] [distortion: %f]" % (epoch, d_loss, g_loss, train_FNR, test_FNR,np.mean(list(itertools.chain(*list_of_distortion))) ))
            # print("%d [train_FNR: %f] " % (epoch,train_FNR))
        end_train = timer()
        del g_loss, d_loss

        print('\ntraining completed in %.3f seconds.\n' % (end_train - start_train))
        print('=============results ============= ')

        print(' attack success rate using train data: {0} \n'
              ' attack success rate using test data: {1}'.format(best_train_FNR, best_test_FNR))
        print('==============================\n ')


        # Plot losses
        plt.figure()
        plt.plot(range(len(self.gloss)), self.gloss, c='r', label='g_loss_rec', linewidth=2)
        plt.plot(range(len(self.dloss)), self.dloss, c='g', linestyle='--', label='d_loss', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('loss')
        plt.legend()
        plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/GAN_Epoch_loss({0}).png'.format(round))
        # plt.show()
        plt.close()

        # Plot TPR
        # plt.figure()
        # plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Training Set', linewidth=2)
        # # plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Attack success rate', linewidth=2)
        # plt.plot(range(len(Test_FNR)), Test_FNR, c='g', linestyle='--', label='Test Set', linewidth=2)
        # plt.xlabel('Epoch')
        # plt.ylabel('FNR')
        # plt.legend()
        # # plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/Epoch_FNR({0}).png'.format(round))
        # plt.show()
        # plt.close()


        return [best_train_FNR, best_test_FNR, (end_train - start_train)]
        # return Train_FNR
