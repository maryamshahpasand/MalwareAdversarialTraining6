import pickle

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import linear_model, svm, tree
from sklearn.model_selection import train_test_split
from timeit import default_timer as timer
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import sklearn

def retrain_classifier(classifier,data_size, _GAN, data, round ,por):
    print('\n\n--- Round: {0} '.format(round+1))

    print('\n\nADVESARIAL RETRAINING...\n\n')

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data[0], data[1], data[2], data[3]

    xtest = np.concatenate([xtsmal, xtsben])
    ytest = np.concatenate([ytsmal, ytsben])
    xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # Generate Train Adversarial Examples
    print('Generating Train Adversarial Examples...\n')
    noise = np.random.uniform(0, 1, (data_size[0], _GAN.noise_size))
    # _GAN.G = torch.load('/home/maryam/Code/python/adversarial_training/torch_impl/_'+str(por)+'/malgan{0}.pt'.format(round))
    with torch.no_grad():
        # not to generate adv version from other avd samples
        gen_examples = _GAN.G([torch.from_numpy(data[0][0][:data_size[0]]).float().cuda(),
                               torch.from_numpy(noise).float().cuda()]).detach()
    x_gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) * (gen_examples.cpu().detach().numpy() > 0.5)

    # print(data[0][0][:data_size[0]].sum(axis=0) - x_gen_examples.sum(axis=0).transpose())


    y_gen_examples = np.ones(len(gen_examples.cpu().detach().numpy()))
    train_gen_examples, test_gen_examples, y_train_gen_examples, y_test_gen_examples = train_test_split(x_gen_examples,
                                                                                                        y_gen_examples,
                                                                                                        test_size=0.20)
    data_size.append(len(gen_examples))
    print('==============new data============ ')
    print(
        'number of original malware = {3}\n'
        'number of adversarial samples (old+new) = {4}\n'
        'number of new adversarial samples = {0}\n'
        'Malware to benign Ratio: {5:.2f}\n'
        'adversarial samples portion of malware samples = {1:.2f}\n'
        'adversarial samples portion of all samples = {2:.2f} '
            .format(len(gen_examples),
                    (np.sum(data_size) - data_size[0]) / np.sum(data_size),
                    (np.sum(data_size) - data_size[0]) / (np.sum(data_size) + len(data[1][0])),
                    data_size[0],
                    np.sum(data_size) - data_size[0],
                    len(data[0][0]) / len(data[1][0])
                    ))
    print('==================================\n ')
    # #select the closest adv samples to original samples to add to training set
    # orig_adv_dist = np.diag(sklearn.metrics.pairwise.euclidean_distances(gen_examples, data[0][0][:data_size[0]]))
    # orig_adv_dist_train = np.diag(sklearn.metrics.pairwise.manhattan_distances(gen_examples, data[0][0][:data_size[0]]))
    # # select best 20 percent of adversarial samples to add to training set and teach the classifier
    # # portion_to_add_to_train_data = np.int(0.2 * len(orig_adv_dist))
    # portion_to_add_to_train_data = np.int(1.0 * len(orig_adv_dist))
    # indext_of_best_adv_samples = np.argpartition(orig_adv_dist, portion_to_add_to_train_data)[:portion_to_add_to_train_data]
    # adv_to_add_to_train = gen_examples[indext_of_best_adv_samples]

    print('retraining classifier...\n')
    start_train = timer()



    classifier.fit(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                   np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))

    pickle.dump(classifier, open('/home/maryam/Code/python/adversarial_training/torch_impl/SVM_'+str(por)+'.sav','wb'))
    end_train = timer()
    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('\n=============results ============= ')

    # Compute Train TPR
    train_TPR = classifier.score(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    conf_matx = confusion_matrix(np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]), classifier.predict(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]))
                                 )
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1],
                                                                         conf_matx[1][0],
                                                                         conf_matx[1][1]))
    train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    # Compute Test
    test_TPR = classifier.score(np.concatenate([xtest_mal, xtest_ben, test_gen_examples]),
                                np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))

    conf_matx = confusion_matrix(np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]) , classifier.predict(np.concatenate([xtest_mal, xtest_ben, test_gen_examples])))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Clean_test_TPR = classifier.score(xtest, ytest)

    conf_matx = confusion_matrix(ytest, classifier.predict(xtest) )
    print('Clean Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    Clean_test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
    Clean_test_FPR = conf_matx[0][1] / (conf_matx[0][1] + conf_matx[0][0])
    print(
        'Train accuracy: {0}\n Test accuracy: {1}  \n FNR for train: {2} \n FNR for test : {3} \n Clean_test_TPR: {4} \n Clean_test_FNR: {5}  \n Clean_test_FPR: {6}   '
            .format(train_TPR, test_TPR, train_FNR, test_FNR, Clean_test_TPR, Clean_test_FNR, Clean_test_FPR))
    print('==============================\n ')

    # keeping the generated samples from previous rounds
    data[0] = (np.concatenate([data[0][0], x_gen_examples]), np.concatenate([data[0][1], np.ones(len(x_gen_examples))]))

    # plot adv and orig samples distance
    # orig_adv_dist = np.diag(
    #                     sklearn.metrics.pairwise.manhattan_distances(torch.round(g_fake_data).cpu().detach().numpy(),
                    #                                                  xmal_batch))
                    # # g_loss_distance = self.criterion_(torch.from_numpy((normalize(orig_adv_dist.reshape(-1, 1))).ravel()).float().cuda(), torch.zeros(orig_adv_dist.shape[0]).cuda())
                    # g_loss_distance = self.criterion_(torch.from_numpy(orig_adv_dist).float().cuda(),
                    #                                   torch.zeros(orig_adv_dist.shape[0]).cuda().view(-1, 1))
    orig_adv_dist = np.diag(sklearn.metrics.pairwise.manhattan_distances(x_gen_examples, data[0][0][:data_size[0]]))
    distortion = orig_adv_dist[range(len(gen_examples) - 1)]

    # plt.legend()
    # plt.show()
    # print(orig_adv_dist)

    return [train_TPR , test_TPR , Clean_test_TPR , train_FNR ,test_FNR, Clean_test_FNR ,Clean_test_FPR, (end_train - start_train)] , data , distortion , classifier

