import numpy as np
import sys
import psutil
import os
import csv
import random
from timeit import default_timer as timer
import pickle
from os import listdir
from os.path import isfile, join
from sklearn.model_selection import KFold
import torch
import torch.nn as nn
# from torchvision import transforms
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data_utils
import re
from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model.tests.test_huber import test_huber_max_iter
from sklearn.neural_network import MLPClassifier
from sklearn import linear_model, svm, tree

from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedShuffleSplit
import collections, functools, operator
import sklearn
import scipy.sparse
import pandas as pd

from sklearn.metrics import roc_curve, auc


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, patience=7, verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model, round):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, round)
        elif score <= self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, round)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, round):
        '''Saves model when validation loss decrease.'''
        if self.verbose:
            # classifier_name = re.split('\W+',str(type(_classifier.estimator)))[-2]
            print(f'Validation loss decreased ({self.val_loss_min:.15f} --> {val_loss:.15f}).  Saving model ...')
        # torch.save(model.state_dict(), './SVM_all_ES_IAR/checkpoint_'+classifier_name+'.pt')
        torch.save(model.state_dict(), './SVM_all_ES_IAR/checkpoint_' + str(round) + '.pt')
        self.val_loss_min = val_loss


def cpuStats():
    print(sys.version)
    print(psutil.cpu_percent())
    print(psutil.virtual_memory())  # physical memory usage
    pid = os.getpid()
    py = psutil.Process(pid)
    memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think
    print('memory GB:', memoryUse)


class surrogate_model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(surrogate_model, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.sigmoid(self.map1(x))
        return torch.sigmoid(self.map2(x))


class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, hidden_size)
        self.map3 = nn.Linear(hidden_size, output_size)
        # self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x_and_example):
        x = torch.cat((x_and_example[0], x_and_example[1]), 1)
        x = torch.sigmoid(self.map1(x))
        x = torch.sigmoid(self.map2(x))
        x = torch.sigmoid(self.map3(x))

        # return torch.max(x_and_example[0], (x*x_and_example[2]))
        return torch.max(x_and_example[0], x)


class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.map1 = nn.Linear(input_size, hidden_size)
        self.map2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.sigmoid(self.map1(x))
        return torch.sigmoid(self.map2(x))


class GAN():
    def __init__(self, input_size, hidden_size, noise_size):
        self.otiginal_data_size = 0
        self.noise_size = noise_size
        self.gloss = []
        self.dloss = []
        self.criterion = nn.BCELoss()
        self.criterion_ = nn.L1Loss()
        self.D = Discriminator(input_size=input_size, hidden_size=hidden_size, output_size=1).cuda()
        self.d_optimizer = optim.Adam(self.D.parameters(), lr=0.001, betas=(0.9, 0.999))
        # self.F = surrogate_model(input_size=input_size, hidden_size=hidden_size, output_size=1).cuda()
        # self.f_optimizer = optim.Adam(self.F.parameters(), lr=0.001, betas=(0.9, 0.999))
        self.G = Generator(input_size=input_size + self.noise_size, hidden_size=hidden_size,
                           output_size=input_size).cuda()
        self.g_optimizer = optim.Adam(self.G.parameters(), lr=0.001, betas=(0.9, 0.999))

    def plot_added_featues(self, list_of_added_features, round):
        frams = pd.DataFrame(list_of_added_features)
        # print(frams.sum(axis=0) / len(list_of_added_features))
        frams.to_csv('./models/list_of_added_features' + str(round) + '.csv')
        ax = frams.plot.bar(stacked=True)
        ax.set_xlabel('EPOCH')
        # for rowNum, row in frams.iterrows():
        #     ypos = 0
        #     featuer=0
        #     for val in row:
        #         if featuer!=6:
        #             ypos += val
        #             ax.text(rowNum, ypos , "{0:.2f}".format(val), color='black' ,ha='center')
        #         featuer+=1
        #     ypos = 0
        plt.title('Average Number of Added Features in Each Epoch')
        plt.savefig('./SVM_all_ES_IAR/added_featues({0}).png'.format(round))

        # chartBox = ax.get_position()
        # ax.set_position([chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])
        # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.8), ncol=1)
        # plt.show()
        plt.close()

    def process_batch(self, local_batch, local_lable):

        xmal_batch = local_batch[(local_lable != 0).nonzero()]
        # print(xmal_batch.shape)

        if (len(xmal_batch.shape) > 2):
            xmal_batch = xmal_batch.reshape((xmal_batch.shape[0] * xmal_batch.shape[1]), xmal_batch.shape[2])
            # print(xmal_batch.shape)

        xben_batch = local_batch[(local_lable == 0).nonzero()]
        # print(xben_batch.shape)
        if len(xben_batch.shape) > 2:
            xben_batch = xben_batch.reshape((xben_batch.shape[0] * xben_batch.shape[1]), xben_batch.shape[2])
        # print(xben_batch.shape)

        return xmal_batch.float(), xben_batch.float()

    def check_added_features(self, x, features):
        added_features = []
        for i in range(len(x)):
            unique, counts = np.unique(
                np.where(x.cpu().numpy()[i].astype(int) > 0, features[:, 2], np.zeros(features[:, 2].shape)),
                # np.where(x.cpu()[i] > 0, features[:, 2], np.zeros(features[:, 2].shape)),
                return_counts=True)
            unique = unique[1:]
            counts = counts[1:]
            added_features.append(dict(zip(unique, counts)))

        batch_added_features = dict(functools.reduce(operator.add, map(collections.Counter, added_features)))
        batch_added_features.update({n: batch_added_features[n] / len(x) for n in batch_added_features.keys()})
        return batch_added_features

    def train(self, _classifier, alpha, epochs, data_original, batch_size=32, round=-1):
        # Load and Split the dataset

        # (x_org_mal, y_org_mal)= data[0][0][:data_size[0]], data[0][1][:data_size[0]]
        # (x_org_ben, y_org_ben) = data[1][0][:data_size[0]], data[1][1][:data_size[0]],
        # (xmal, ymal), (xben, yben) =  data[0], data[1]
        #
        # X_org = np.concatenate([x_org_mal, x_org_ben])
        # Y_org = np.concatenate([y_org_mal, y_org_ben])
        #
        # X_org_adv = np.concatenate([xben, xmal])
        # Y_org_adv = np.concatenate([yben, ymal])
        # xtest, ytest =  data[2][0], data[2][1]
        early_stopping = EarlyStopping(patience=20, verbose=True)

        # create manifest feature mask
        features = np.load('./MalwareDataset/Drebin_important_features.npy')
        features = np.append(features, [features[len(features) - 1]], axis=0)
        manifest_features = np.where(
            (features[:, 2] == 'activity') | (features[:, 2] == 'feature') | (features[:, 2] == 'permission') | (
                    features[:, 2] == 'service_receiver') | (features[:, 2] == 'provider') | (
                    features[:, 2] == 'service') | (features[:, 2] == 'intent'), np.ones(len(features)),
            np.zeros(len(features)))
        manifest_features = torch.from_numpy(manifest_features).float().cuda()

        (xmal, ymal), (xben, yben) = data_original[0], data_original[1]
        X = np.concatenate([xben, xmal])
        Y = np.concatenate([yben, ymal])
        xtrain, ytrain, xtest_mal, xtest_ben = X, Y, data_original[2][0], data_original[2][1]

        train = data_utils.TensorDataset(torch.from_numpy(xtrain), torch.from_numpy(ytrain))
        num_train = len(train)
        indices = list(range(num_train))
        np.random.shuffle(indices)
        split = int(np.floor(0.2 * num_train))
        train_idx, valid_idx = indices[split:], indices[:split]
        train_sampler = data_utils.sampler.WeightedRandomSampler(train_idx, len(train_idx))
        valid_sampler = data_utils.sampler.WeightedRandomSampler(valid_idx, len(valid_idx))

        # train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=sampler)
        train_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=train_sampler)
        valid_loader = data_utils.DataLoader(train, batch_size=batch_size, sampler=valid_sampler)

        print('\nTRAINING GAN...\n')
        self.gtrainloss, self.gvalidloss, self.dloss = [], [], []
        self.avg_train_losses = []
        self.avg_valid_losses = []

        list_of_added_features = []

        Train_FNR, Test_FNR = [], []
        best_test_epoch, best_train_epoch = 0.0, 0.0
        train_distortion = 542
        test_distortion = 542
        best_test_FNR, best_train_FNR = 0.0, 0.0
        sum_test_distortion = 0
        test_FNR, train_FNR = 0.0, 0.0
        min_distortion = 243
        start_train = timer()
        for epoch in range(epochs):
            batch_added_features = []
            for local_batch, local_lable in train_loader:
                # ---------------------
                #  Train substitute_detector
                # ---------------------

                xmal_batch, xben_batch = self.process_batch(local_batch, local_lable)
                y_predict = _classifier.predict(xmal_batch)
                # xmal_batch = xmal_batch[y_predict == 1]
                if xmal_batch.shape[0] < 1 or xben_batch.shape[0] < 1:
                    break
                else:

                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                    yclassifierben_batch = _classifier.predict(xben_batch)

                    # Generate a batch of new malware examples
                    gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda(), manifest_features]).detach()
                    # gen_examples = self.G([xmal_batch.float().cuda(), noise.float().cuda()]).detach()
                    batch_added_features.append(
                        self.check_added_features((torch.ones(gen_examples.shape).cuda() * (
                                    gen_examples.cuda() > 0.5).float()) - xmal_batch.cuda().float(), features))
                    yganmal_batch = _classifier.predict(
                        np.ones(gen_examples.cpu().detach().numpy().shape) * (
                                    gen_examples.cpu().detach().numpy() > 0.5))
                    # ---------------------
                    #  Train Surrogate model
                    # ---------------------

                    # f_fake_lable = self.F(torch.ones(gen_examples.shape).cuda() * (gen_examples > 0.5).float().cuda())
                    # f_loss_fake = self.criterion(f_fake_lable.squeeze(), torch.from_numpy(yganmal_batch).float().cuda())
                    #
                    # f_real_lable = self.F(xben_batch.float().cuda())
                    # f_loss_real = self.criterion(f_real_lable.squeeze(), torch.from_numpy(yclassifierben_batch).float().cuda())
                    # f_loss = 0.5 * torch.add(f_loss_real, f_loss_fake)
                    #
                    # self.F.zero_grad()
                    # f_loss.backward(retain_graph=True)
                    # self.f_optimizer.step()

                    # ---------------------
                    #  Train Discriminator
                    # ---------------------
                    d_fake_decision = self.D(
                        torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float())
                    d_loss_fake = self.criterion(d_fake_decision.squeeze(),
                                                 torch.from_numpy(yganmal_batch).float().cuda())
                    # d_loss_fake = self.criterion(d_fake_decision.squeeze(), torch.zeros(d_fake_decision.shape[0]).float().cuda())
                    d_real_decision = self.D(xben_batch.float().cuda())
                    d_loss_real = self.criterion(d_real_decision.squeeze(),
                                                 torch.from_numpy(yclassifierben_batch).float().cuda())
                    # d_loss_real = self.criterion(d_real_decision.squeeze(), torch.ones(d_real_decision.shape[0]).float().cuda())
                    d_loss = 0.5 * torch.add(d_loss_real, d_loss_fake)

                    self.D.zero_grad()
                    d_loss.backward(retain_graph=True)
                    self.d_optimizer.step()

                    # ---------------------
                    #  Train Generator
                    # ---------------------

                    noise = torch.rand(xmal_batch.shape[0], self.noise_size)

                    # Train the generator
                    g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda(), manifest_features])
                    # g_fake_data = self.G([xmal_batch.float().cuda(), noise.float().cuda()])
                    dg_fake_decision = self.D(g_fake_data)
                    g_loss_discriminator = self.criterion(dg_fake_decision.squeeze(),
                                                          torch.zeros(xmal_batch.shape[0]).cuda())

                    # distance = torch.mean(g_fake_data - xmal_batch.float().cuda(), dim=1)
                    # g_loss_distance = self.criterion(distance, torch.zeros(distance.shape).cuda())
                    ## g_loss_distance.requires_grad = True

                    ## f_fake_lable= self.F(torch.ones(g_fake_data.shape).cuda() * (g_fake_data > 0.5).float().cuda())
                    ## g_loss_classifier =  self.criterion(f_fake_lable , torch.zeros(f_fake_lable.shape).float().cuda())

                    ## g_loss = 0.9999*g_loss_samples+ 0.0001*g_loss_distance
                    # g_loss = 0.0001*g_loss_discriminator+ 0.9999*g_loss_distance
                    g_loss = g_loss_discriminator
                    # #g_loss =  alpha* g_loss_discriminator + (1 - alpha) * g_loss_distance
                    # g_loss =  (1 - alpha) * g_loss_discriminator +alpha * g_loss_distance

                    self.G.zero_grad()
                    # ## g_loss_classifier.backward()
                    # g_loss_distance.backward(retain_graph=True)
                    # g_loss_discriminator.backward(retain_graph=True)
                    g_loss.backward()

                    self.g_optimizer.step()
                    self.gtrainloss.append(g_loss)

                    torch.cuda.empty_cache()

            end_train = timer()

            for data, target in valid_loader:
                noise = torch.rand(xmal_batch.shape[0], self.noise_size)
                g_fake_data = self.G(
                    [xmal_batch.float().cuda(), noise.float().cuda(), manifest_features]).detach()
                dg_fake_decision = self.D(g_fake_data)
                g_loss_samples = self.criterion(dg_fake_decision.squeeze(), torch.zeros(xmal_batch.shape[0]).cuda())
                # calculate the loss
                # record validation loss
                self.gvalidloss.append(g_loss_samples)
            self.avg_train_losses.append(torch.mean(torch.stack(self.gtrainloss)))
            self.avg_valid_losses.append(torch.mean(torch.stack(self.gvalidloss)))

            early_stopping(torch.mean(torch.stack(self.gvalidloss)), self.G, round)
            if early_stopping.early_stop:
                print("Early stopping")
                break
            self.gtrainloss = []
            self.gvalidloss = []
            self.dloss.append(d_loss)

            self.dloss.append(d_loss)
            list_of_added_features.append(pd.DataFrame(batch_added_features).mean(axis=0).to_dict())
            # ======Compute attack success rate on train data
            xtrain_mal = xtrain[(ytrain == 1).nonzero()[0]]
            noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.noise_size))
            gen_examples = self.G(
                [torch.from_numpy(xtrain_mal).float().cuda(), torch.from_numpy(noise).float().cuda(),
                 manifest_features])

            # [torch.from_numpy(xtrain_mal).float().cuda(), torch.from_numpy(noise).float().cuda()])
            gen_examples = np.ones(gen_examples.cpu().detach().numpy().shape) * (
                    gen_examples.cpu().detach().numpy() > 0.5)

            if (sum(np.ones(gen_examples.shape[0]) - classifier_.predict(gen_examples)) != 0):
                conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
            else:
                train_FNR = 0

            if train_FNR > best_train_FNR:
                best_train_FNR = train_FNR
                train_distortion = pd.DataFrame(batch_added_features).mean(axis=0).sum()
                best_train_epoch = epoch
                # ======Compute attach success rate on test data
                y_classifier_predict = classifier_.predict(xtest_mal)
                # remove classifier false negatives
                xtest_mal = xtest_mal[(y_classifier_predict != 0).nonzero()[0]]
                noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.noise_size))
                xtest_mal_cuda = torch.from_numpy(xtest_mal).float().cuda()
                gen_examples_ = self.G(
                    [xtest_mal_cuda, torch.from_numpy(noise).float().cuda(), manifest_features])
                gen_examples = np.ones(gen_examples_.cpu().detach().numpy().shape) * (
                        gen_examples_.cpu().detach().numpy() > 0.5)
                if (sum(np.ones(gen_examples.shape[0]) - classifier_.predict(gen_examples)) != 0):
                    conf_matx = confusion_matrix(np.ones(gen_examples.shape[0]), classifier_.predict(gen_examples))
                    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])
                else:
                    test_FNR = 0
                Test_FNR.append(test_FNR)

                if test_FNR > best_test_FNR:
                    best_test_FNR = test_FNR
                    test_distortion = self.check_added_features(torch.from_numpy(gen_examples).float().cuda() * (
                            gen_examples_ > 0.5).float() - xtest_mal_cuda,
                                                                features)
                    sum_test_distortion = sum(test_distortion.values())
                    best_test_epoch = epoch

                    # torch.save(self.G,
                    #           './SVM_all_ES_IAR/svmmalgan{0}.pt'.format(
                    #               round))
                    Test_FNR.append(test_FNR)

            # # if min_distortion > pd.DataFrame(batch_added_features).mean(axis=0).sum():
            # #     min_distortion = pd.DataFrame(batch_added_features).mean(axis=0).sum()
            #
            # if train_FNR > best_train_FNR:
            #     best_train_FNR = train_FNR
            # # if test_FNR >= best_test_FNR and min_distortion>=pd.DataFrame(batch_added_features).mean(axis=0).sum():
            # #     # best_test_FNR = test_FNR
            # #     # min_distortion = pd.DataFrame(batch_added_features).mean(axis=0).sum()
            # #     epoch_ = epoch
            # elif test_FNR >= best_test_FNR:
            #     best_test_FNR = test_FNR
            #     # print('saving mulgan weights at epoch:', epoch)
            #
            #     # print("[D loss: %f] [G loss: %f] \n" % (self.dloss[-1], self.gloss[-1]))
            #     torch.save(self.G,
            #                '/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/malgan{0}.pt'.format(
            #                    round))
            # # if torch.mean((distance*542))<max_distortion :
            # #     max_distortion=torch.mean((distance*542))
            #
            # # Print the progress
            # # if epoch % 20 == 0:
            # # print(pd.DataFrame(batch_added_features).mean(axis=0))
            # # print("%d [D loss: %f] [G sample loss: %f]  [G discriminator loss: %f]      <<[test_FNR: %f] [train_FNR: %f]>> [disstortion: %f]" % (epoch , d_loss, g_loss_distance, g_loss_samples, test_FNR,train_FNR , pd.DataFrame(batch_added_features).mean(axis=0).sum()))
            # if epoch%5==0:
            print(
                "%d [D loss: %f] [G sample loss: %f] [G discriminator loss: %f] <<[test_FNR: %f] [train_FNR: %f]>> [disstortion: %f]" % (
                    epoch, d_loss, g_loss_discriminator, g_loss, test_FNR, train_FNR,
                    pd.DataFrame(batch_added_features).mean(axis=0).sum()))
        end_train = timer()
        del g_loss, d_loss
        # torch.save(self.G,
        #            './SVM_all_ES_IAR/svmmalgan{0}.pt'.format(round))
        # print('\ntraining completed in %.3f seconds.\n' % (end_train - start_train))
        # print('=============results ============= ')
        #
        # print(' attack success rate using train data: {0} \n'
        #       ' attack success rate using test data: {4} distortion {5}\n '
        #       'best attack success rate using test data: {1} best distortion {2} epoch {3}'.format(best_train_FNR,
        #                                                                                            best_test_FNR,
        #                                                                                            min_distortion,
        #                                                                                            epoch, test_FNR,
        #                                                                                            pd.DataFrame(
        #                                                                                                batch_added_features).mean(
        #                                                                                                axis=0).sum()))
        # print(self.check_added_features(
        #     torch.from_numpy(gen_examples).float().cuda() - torch.from_numpy(xtest_mal).float().cuda(), features))
        #
        print('training completed in %.2f secound...', end_train - start_train)
        print('==============================\n ')
        print('best attack success rate using train data: {0:.2f} train distortion {1:.2f} at epoch {2} \n'
              'best attack success rate using test  data: {3:.2f} test distortion {4:.2f} epoch {5} detailed distortion: {6} '.format(
            best_train_FNR * 100, train_distortion, best_train_epoch,
            best_test_FNR * 100, sum_test_distortion, best_test_epoch, test_distortion))
        # print(self.check_added_features(
        #     xtest_mal_cuda * (gen_examples_ > 0.5).float() - xtest_mal_cuda, features))

        print('==============================\n ')

        self.plot_added_featues(list_of_added_features, round)

        # # Plot losses
        # plt.figure()
        # plt.plot(range(len(self.gloss)), self.gloss, c='r', label='g_loss_rec', linewidth=2)
        # plt.plot(range(len(self.dloss)), self.dloss, c='g', linestyle='--', label='d_loss', linewidth=2)
        # plt.xlabel('Epoch')
        # plt.ylabel('loss')
        # plt.legend()
        # plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/GAN_Epoch_loss({0}).png'.format(round))
        # # plt.show()
        # plt.close()
        #
        # # Plot TPR
        # plt.figure()
        # plt.plot(range(len(Train_FNR)), Train_FNR, c='r', label='Training Set', linewidth=2)
        # plt.plot(range(len(Test_FNR)), Test_FNR, c='g', linestyle='--', label='Validation Set', linewidth=2)
        # plt.xlabel('Epoch')
        # plt.ylabel('FNR')
        # plt.legend()
        # plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/Epoch_FNR({0}).png'.format(round))
        # # plt.show()
        # plt.close()

        # return [best_train_FNR, best_test_FNR]
        return [best_train_FNR, best_test_FNR, train_distortion, sum_test_distortion], (end_train - start_train)


def build_classifier(type='RF', data=[]):
    if type is 'RF':
        Parameters = {'n_estimators': [100], 'max_depth': [50]}
        model = GridSearchCV(RandomForestClassifier(), Parameters, cv=5, scoring='f1', n_jobs=-1)

        # model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=1)
    elif type is 'SVM':

        Parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
        model = GridSearchCV(LinearSVC(), Parameters, cv=5, scoring='f1', n_jobs=-1)

        # model = svm.SVC
    elif type is 'RBF_SVM':
        # C_range = np.logspace(-2, 10, 13)
        # gamma_range = np.logspace(-9, 3, 13)
        # param_grid = dict(gamma=gamma_range, C=C_range)
        # cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
        # model = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
        # # Parameters={'kernel':['rbf'],'gamma': [1e-3, 1e-4],'C': [0.001, 0.01, 0.1]}
        # # model = GridSearchCV(svm.SVC(), Parameters, cv=5, scoring='f1', n_jobs=-1)
        model = svm.SVC(kernel='rbf')
    elif type is 'LR':
        model = linear_model.LogisticRegression()
    elif type is 'DT':
        model = tree.DecisionTreeRegressor()
    elif type is 'MLP':
        model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                              solver='sgd', verbose=0, tol=1e-4, random_state=1,
                              learning_rate_init=.1)

    # for i in np.arange(4):
    #     x = np.concatenate([np.load('./data_splits/x_train_fold_'+str(i)+'.npy'),np.load('./data_splits/x_test_fold_'+str(i)+'.npy')])
    #     y = np.concatenate([np.load('./data_splits/y_train_fold_'+str(i)+'.npy'),np.load('./data_splits/y_test_fold_'+str(i)+'.npy')])
    #     model.fit(x, y)

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data[0], data[1], data[2], data[3]
    # xtrain = scipy.sparse.vstack((xben, xmal))
    # ytrain = np.concatenate([yben, ymal])
    # xtest = scipy.sparse.vstack((xtsmal, xtsben))
    # ytest = np.concatenate([ytsmal, ytsben])
    xtrain = np.concatenate([xben, xmal])
    ytrain = np.concatenate([yben, ymal])
    xtest = np.concatenate([xtsmal, xtsben])
    ytest = np.concatenate([ytsmal, ytsben])

    # print('\n\n--- Round: {0} '.format(0))
    #
    # print('=============data============= ')
    # print(
    #     'number of original malware = {0}\n'
    #     'number of adversarial samples (old+new) = {1}\n'
    #     'number of new adversarial samples = {2}\n'
    #     'Malware to benign Ratio: {3:.2f}\n'
    #     'adversarial samples portion of malware samples = {4:.2f}\n'
    #     'adversarial samples portion of all samples = {5:.2f} '
    #         # .format(len(xmal) + len(data[2][0][(data[2][1]==1).nonzero()[0]]),
    #         .format(xmal.shape[0],
    #                 0,
    #                 0,
    #                 xmal.shape[0] / xben.shape[0],
    #                 0,
    #                 0
    #                 ))
    # print('==============================\n ')

    start_train = timer()
    # # model = pickle.load(open('./models/male_qabl/SVM_0.5AtRound_5.sav', 'rb'))
    # if os.path.isfile('./models/male_qabl/' + type + '__' + str(por) + '.sav'):
    if os.path.isfile('./models/male_qabl/' + type + '.sav'):
        print('LOADING CLASSIFIER...\n')
        # if os.path.isfile('/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav'):
        # model = pickle.load(open('./models/male_qabl/' + type + '__' + str(por) + '.sav', 'rb'))
        model = pickle.load(open('./models/male_qabl/' + type + '.sav', 'rb'))
        # '/home/maryam/Code/python/adversarial_training/torch_impl/Different_ratio/models/'+type+'weighted__'+por+'.sav', 'rb'))
    else:
        print('TRAINING CLASSIFIER...\n')
        model.fit(xtrain, ytrain)
        # print(model.cv_results_.)
        #     BestModel = svm_model.best_estimator_
        # print("Best Model Selected : {}".format(BestModel))
        pickle.dump(model, open('./models/male_qabl/' + type + '__' + str(por) + '.sav', 'wb'))
    end_train = timer()

    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('=============results ============= ')

    conf_matx = confusion_matrix(ytrain, model.predict(xtrain))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1],
                                                                         conf_matx[1][0],
                                                                         conf_matx[1][1]))
    train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    conf_matx = confusion_matrix(ytest, model.predict(xtest))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Original_Train_TPR = model.score(xtrain, ytrain)
    Original_Test_TPR = model.score(xtest, ytest)
    print(
        'Original_Train_TPR: {0:.4f}, Original_Test_TPR: {1:.4f} , FNR for train: {2:.2f} , FNR for test: {3:.2f}'.format(
            Original_Train_TPR, Original_Test_TPR, train_FNR, test_FNR))
    print('==============================\n ')
    # fpr, tpr, _ = roc_curve(ytest, model.decision_function(xtest))
    # # fpr, tpr, _ = roc_curve(ytest, model.predict_proba(xtest)[:,1])
    # roc_auc = auc(fpr, tpr)
    accuracy = accuracy_score(ytest, model.predict(xtest))
    f1 = f1_score(ytest, model.predict(xtest))
    precision = precision_score(ytest, model.predict(xtest))
    # roc_curve_values.append()
    # return model , [Original_Train_TPR , Original_Test_TPR , Original_Test_TPR , train_FNR ,test_FNR, test_FNR ,accuracy,f1,precision],[ fpr, tpr, roc_auc ]
    return model, [Original_Train_TPR, Original_Test_TPR, Original_Test_TPR, train_FNR, test_FNR, test_FNR, accuracy,
                   f1, precision], [1, 1, 1]


def retrain_classifier(classifier, _GAN, data_, round):
    # create manifest feature mask
    features = np.load(
        './MalwareDataset/Drebin_important_features.npy')
    features = np.append(features, [features[len(features) - 1]], axis=0)
    manifest_features = np.where(
        (features[:, 2] == 'activity') | (features[:, 2] == 'feature') | (features[:, 2] == 'permission') | (
                features[:, 2] == 'service_receiver') | (features[:, 2] == 'provider') | (
                features[:, 2] == 'service') | (features[:, 2] == 'intent'), np.ones(len(features)),
        np.zeros(len(features)))
    manifest_features = torch.from_numpy(manifest_features).float().cuda()

    print('\n\n--- Round: {0} '.format(round + 1))

    print('\n\nADVESARIAL RETRAINING...\n\n')

    (xmal, ymal), (xben, yben), (xtsmal, xtsben), (ytsmal, ytsben) = data_[0], data_[1], data_[2], data_[3]
    xtest = np.concatenate([xtsmal, xtsben])
    ytest = np.concatenate([ytsmal, ytsben])
    xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # Generate Train Adversarial Examples
    print('Generating Train Adversarial Examples...\n')
    noise = np.random.uniform(0, 1, (data_size[0], _GAN.noise_size))
    # _GAN.G = torch.load('./SVM_all_ES_IAR/checkpoint_{0}.pt'.format(round))
    _GAN.G.load_state_dict(torch.load('./SVM_all_ES_IAR/checkpoint_{0}.pt'.format(round)))
    _GAN.G.eval()
    with torch.no_grad():
        # not to generate adv version from other avd samples
        gen_examples = _GAN.G(
            [torch.from_numpy(data_[0][0][:data_size[0]]).float().cuda(), torch.from_numpy(noise).float().cuda(),
             manifest_features]).detach()
        # torch.from_numpy(noise).float().cuda()]).detach()
    x_gen_examples = (torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()).cpu().numpy()
    gen_examples = (torch.ones(gen_examples.shape).cuda() * (gen_examples.cuda() > 0.5).float()).cpu().numpy()

    # adding adversarial samples to training set only
    y_predict = classifier.predict(x_gen_examples)
    x_gen_examples = x_gen_examples[y_predict == 0]
    x_gen_examples = np.unique(x_gen_examples, axis=0)

    y_gen_examples = np.ones(len(x_gen_examples))

    train_gen_examples, test_gen_examples, y_train_gen_examples, y_test_gen_examples = train_test_split(x_gen_examples,
                                                                                                        y_gen_examples,
                                                                                                        test_size=0.20)
    # save samples to check among rounds
    np.save('./SVM_all_ES_IAR/x_gen_examples_' + str(round) + '.npy', x_gen_examples)
    data_size.append(len(x_gen_examples))
    print('data size:', data_size)

    (xben, yben) = (xben[0:sum(data_size)], yben[0:sum(data_size)])

    xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)

    # keeping the generated samples from previous rounds
    print('np.unique(x_gen_examples,axis=0).shape', np.unique(x_gen_examples, axis=0).shape)
    print('np.unique(data_[0][0],axis=0).shape', np.unique(data_[0][0], axis=0).shape)
    print('keeping the generated samples from previous rounds')
    data_[0] = (
    np.concatenate([data_[0][0], x_gen_examples]), np.concatenate([data_[0][1], np.ones(len(x_gen_examples))]))
    print('np.unique(data_[0][0],axis=0).shape', np.unique(data_[0][0], axis=0).shape)

    # print('uniqe samples:')
    # for i in range(1,len(data_size)):
    #     print('round ',i,':',len(np.unique(np.concatenate([x_gen_examples,data[0][0][np.cumsum(data_size)[i-1]:np.cumsum(data_size)[i]]]), axis=0)),' out of ', data_size[i-1]+data_size[i])
    print('==============new data============ ')
    print(
        'number of original malware = {0}\n'
        'number of adversarial samples (old+new) = {1}\n'
        'number of new adversarial samples = {2}\n'
        'Malware to benign Ratio: {3:.2f}\n'
        'adversarial samples portion of malware samples = {4:.2f}\n'
        'adversarial samples portion of all samples = {5:.2f} '
            .format(data_size[0],
                    sum(data_size),
                    len(x_gen_examples),
                    len(data_[0][0]) / len(data_[1][0]),
                    (np.sum(data_size) - data_size[0]) / np.sum(data_size),
                    (np.sum(data_size) - data_size[0]) / (np.sum(data_size) + len(data_[1][0]))
                    ))
    print('==================================\n ')
    # #select the closest adv samples to original samples to add to training set
    # orig_adv_dist = np.diag(sklearn.metrics.pairwise.euclidean_distances(gen_examples, data[0][0][:data_size[0]]))
    # orig_adv_dist_train = np.diag(sklearn.metrics.pairwise.manhattan_distances(gen_examples, data[0][0][:data_size[0]]))
    # # select best 20 percent of adversarial samples to add to training set and teach the classifier
    # # portion_to_add_to_train_data = np.int(0.2 * len(orig_adv_dist))
    # portion_to_add_to_train_data = np.int(1.0 * len(orig_adv_dist))
    # indext_of_best_adv_samples = np.argpartition(orig_adv_dist, portion_to_add_to_train_data)[:portion_to_add_to_train_data]
    # adv_to_add_to_train = gen_examples[indext_of_best_adv_samples]

    print('retraining classifier...\n')
    x = np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples])
    y = np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples])
    print('on data shape:', x.shape[0])
    print('xtrain_mal shape:', xtrain_mal.shape[0])
    print('xtrain_ben shape:', xtrain_ben.shape[0])
    print('train_gen_examples shape:', train_gen_examples.shape[0])
    start_train = timer()
    Parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'max_iter': [1000, 10000]}
    classifier = GridSearchCV(LinearSVC(), Parameters, cv=5, scoring='f1', n_jobs=-1)

    classifier.fit(x, y)

    # pickle.dump(classifier,open('./models/male_qabl/SVM_' + str(por) + 'AtRound_'+str(round+1)+'.sav','wb'))
    pickle.dump(classifier, open('./SVM_all_ES_IAR/SVMATRound_' + str(round + 1) + '.sav', 'wb'))

    end_train = timer()
    print('training completed in %.3f seconds:' % (end_train - start_train))
    print('\n=============results ============= ')

    # Compute Train TPR
    train_TPR = classifier.score(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples]),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtrain_mal, xtrain_ben, train_gen_examples])),
                                 np.concatenate([ytrain_mal, ytrain_ben, y_train_gen_examples]))
    print('Train Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3}'.format(conf_matx[0][0], conf_matx[0][1],
                                                                         conf_matx[1][0],
                                                                         conf_matx[1][1]))
    train_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    # Compute Test
    test_TPR = classifier.score(np.concatenate([xtest_mal, xtest_ben, test_gen_examples]),
                                np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))

    conf_matx = confusion_matrix(classifier.predict(np.concatenate([xtest_mal, xtest_ben, test_gen_examples])),
                                 np.concatenate([ytest_mal, ytest_ben, y_test_gen_examples]))
    print('Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                          conf_matx[1][0],
                                                                          conf_matx[1][1]))
    test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    Clean_test_TPR = classifier.score(xtest, ytest)

    conf_matx = confusion_matrix(classifier.predict(xtest), ytest)
    print('Clean Test  Confusion :: TN: {0}  FP: {1}  FN: {2}  TP: {3} '.format(conf_matx[0][0], conf_matx[0][1],
                                                                                conf_matx[1][0],
                                                                                conf_matx[1][1]))
    Clean_test_FNR = conf_matx[1][0] / (conf_matx[1][0] + conf_matx[1][1])

    print(
        'Train accuracy: {0:.2f}\n Test accuracy: {1:.2f}  \n FNR for train: {2:.2f} \n FNR for test: {3:.2f} \n Clean_test_TPR: {4:.2f} \n Clean_test_FNR: {5:.2f}  '
            .format(train_TPR, test_TPR, train_FNR, test_FNR, Clean_test_TPR, Clean_test_FNR))
    print('==============================\n ')
    fpr, tpr, _ = roc_curve(ytest, classifier.decision_function(xtest))
    roc_auc = auc(fpr, tpr)
    accuracy = accuracy_score(ytest, classifier.predict(xtest))
    f1 = f1_score(ytest, classifier.predict(xtest))
    precision = precision_score(ytest, classifier.predict(xtest))

    # plot adv and orig samples distance
    orig_adv_dist = np.sum(gen_examples - xmal[:data_size[0]], 1)

    plot_dist.append(orig_adv_dist[range(len(gen_examples) - 1)])
    np.save('./SVM_all_ES_IAR/plot_dist' + str(round) + '.npy', plot_dist)
    #
    # plt.figure()
    # plt.boxplot(plot_dist, labels=[i for i in range(len(plot_dist))])
    #
    # # plt.show()
    #
    # plt.xlabel('round of retraining')#(54 adv. sample in each round is added to dataset)')
    # plt.ylabel('Generated samples disstortion (L1 norm)')
    # plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/new_adv_samples_distortion.png')
    # plt.close()
    # # plt.legend()
    # # plt.show()
    # # print(orig_adv_dist)

    return [train_TPR, test_TPR, Clean_test_TPR, train_FNR, test_FNR, Clean_test_FNR, accuracy, f1, precision], [fpr,
                                                                                                                 tpr,
                                                                                                                 roc_auc], data_, classifier, (
                       end_train - start_train)


def load_data():
    xtrain = scipy.sparse.load_npz('./MalwareDataset/x_train.npz').toarray()
    ytrain = np.load('./MalwareDataset/y_train.npy')
    xtrain_mal = xtrain[np.where(ytrain == 1)]
    xtrain_ben = xtrain[np.where(ytrain == 0)]
    ytrain_mal = ytrain[np.where(ytrain == 1)]
    ytrain_ben = ytrain[np.where(ytrain == 0)]
    # xtrain_mal = xtrain_mal[0:1000]
    # xtrain_ben = xtrain_ben[0:1000]
    # ytrain_mal =ytrain_mal[0:1000]
    # ytrain_ben = ytrain_ben[0:1000]
    xtest = scipy.sparse.load_npz('./MalwareDataset/x_test.npz').toarray()
    ytest = np.load('./MalwareDataset/y_test.npy')
    features = np.load('./MalwareDataset/Drebin_important_features.npy')
    features = np.append(features, [features[len(features) - 1]], axis=0)

    xtest_mal = xtest[np.where(ytest == 1)]
    xtest_ben = xtest[np.where(ytest == 0)]
    ytest_mal = ytest[np.where(ytest == 1)]
    ytest_ben = ytest[np.where(ytest == 0)]

    # xtest_mal = xtest_mal[0:500]
    # xtest_ben =  xtest_ben[0:500]
    # ytest_mal = ytest_mal[0:500]
    # ytest_ben =   ytest_ben[0:500]
    # # data = np.load('/home/maryam/Code/python/Malware-GAN-master/data.npz')
    # xben = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_benign_binary_features.npy')
    # xmal = np.load(
    #     '/home/maryam/Code/python/poisoning_project/datasets/Drebin_Kuafadet/np/x_malware_binary_features.npy')
    # # xmal = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_malware_binary_features.npy')
    # # xben = np.load('/home/maryam/Code/python/poisoning_project/datasets/Drebin_binary_features_11309all_11176unique/x_benign_binary_features.npy')
    #
    # # xmal = xmal[0:200]
    # ymal = np.ones(len(xmal))
    #
    # xben = xben[0:len(xmal)]
    # # xben = xben[0:len(xmal)+1000]
    # yben = np.zeros(len(xben))
    # # xmal, ymal, xben, yben = data['xmal'], data['ymal'], data['xben'], data['yben']
    # # ymal, yben = np.ones(len(xmal)) , np.zeros(len(xben))
    # # same number of malware and bengin samples
    # # return [(xmal[0,200], ymal[0,200]), (xben[0,200], yben[0,200])]
    # # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)
    # xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)
    # # return [(xmal, ymal), (xben[0:len(xmal)], yben[0:len(xmal)])]
    # print(
    #     'number of malware for training = {0}\n'
    #     'number of benign for training= {1}\n'
    #     'number of malware for test  = {2}\n'
    #     'number of benign for test= {3}\n'
    #         .format(len(xtrain_mal),
    #                 len(xtrain_ben),
    #                 len(xtest_mal),
    #                 len(xtest_ben)))
    return [(xtrain_mal, ytrain_mal), (xtrain_ben, ytrain_ben),
            (xtest_mal, xtest_ben), (ytest_mal, ytest_ben)]
    # return [(xmal[0:round(len(xmal)/5)], ymal[0:round(len(ymal)/5)]), (xben, yben)]

def plot_roc(roc_curve_values):
    np.save('./SVM_all_ES_IAR/roc_curve_values.npy', roc_curve_values)
    plt.figure()
    for i in range(len(roc_curve_values)):
        plt.plot(roc_curve_values[i][0], roc_curve_values[i][1],
                 lw=2, label='Round %d (auc = %0.4f)' % (i, roc_curve_values[i][2]))
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic ')
    plt.legend(loc="lower right")
    # plt.savefig('/home/maryam/Code/python/adversarial_training/torch_impl/male_qabl/roc_all.eps'.format(round))

    plt.show()

    plt.close()

    plt.figure()
    plt.plot(roc_curve_values[0][0], roc_curve_values[0][1],
             lw=2, label='Original (auc = %0.4f)' % roc_curve_values[0][2])
    plt.plot(roc_curve_values[-1][0], roc_curve_values[-1][1],
             lw=2, label='Round 10 (auc = %0.4f)' % roc_curve_values[-1][2])
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic ')
    plt.legend(loc="lower right")
    plt.savefig('./SVM_all_ES_IAR/roc_first_last.eps'.format(round))

    plt.show()

    plt.close()


if __name__ == '__main__':

    # frams = pd.read_csv('./models/list_of_added_features.csv', encoding='utf-8')
    # ax = frams.plot.bar(stacked=True)
    # ax.set_xlabel('EPOCH')
    # plt.xticks(np.arange(0,50,  step=10))
    # classifiers = []
    # models_path = './models/SVMmodelsESORICS2020Genralization/'
    # models = [f for f in listdir(models_path) if isfile(join(models_path, f))]
    # for model in models:
    #     classifiers.append(model[0:-4])
    ####generalizability>>>>>>>>>>>

    for classifer_type in ['SVM', 'LR', 'MLP', 'RF', 'RBF_SVM']:
        # for classifer_type in classifiers:#'SVM','RF','RBF_SVM','LR', 'MLP']:
        print('EXPERIMENT ON <<' + classifer_type + '>> :')
        results = []
        data_all = load_data()
        # data_all= load_data.load_data(loaded= True)
        mal_num = data_all[0][0].shape[0]
        ben_num = data_all[1][0].shape[0]
        hist = {}
        hist['0.1'] = []
        hist['0.2'] = []
        hist['0.3'] = []
        hist['0.4'] = []
        hist['0.5'] = []
        por = 0.5  # , 0.4 , 0.3 , 0.2 ,0.1  ]:
        if por > 0:
            porr = int(mal_num / por - mal_num)
        else:
            porr = ben_num - 1
        print(
            'number of malware for training = {0}\n'
            'number of benign for training= {1}\n'
            'Malware portion  = {2}\n'
                .format(mal_num,
                        porr, por))

        data_original = [  # (xmal, ymal)
            data_all[0],
            # (xben,yben )
            (data_all[1][0][0:porr], data_all[1][1][0:porr]),
            # (xtsmal, xtsben)
            (data_all[2][0], data_all[2][1][0:int((data_all[1][0][0:porr].shape[0]) * (0.3 / 0.7))]),
            # portion of train to test
            # (ytsmal, ytsben)
            (data_all[3][0],
             data_all[3][1][0:int((data_all[1][0][0:porr].shape[0]) * (0.3 / 0.7))])]  # portion of train to test
        data_ = [  # (xmal, ymal)
            data_all[0],
            # (xben,yben )
            (data_all[1][0][0:porr], data_all[1][1][0:porr]),
            # (xtsmal, xtsben)
            (data_all[2][0], data_all[2][1][0:int((data_all[1][0][0:porr].shape[0]) * (0.3 / 0.7))]),
            # portion of train to test
            # (ytsmal, ytsben)
            (data_all[3][0],
             data_all[3][1][0:int((data_all[1][0][0:porr].shape[0]) * (0.3 / 0.7))])]  # portion of train to test

        # sarogate_model = surrogate_model(data_original[0][0].shape[1], 200, 1)
        plot_dist = []
        data_size = [len(data_original[0][0])]
        classifier_, result_cls, roc = build_classifier(type=classifer_type, data=data_original)
        roc_curve_values = [roc]
        # alphas = [0.1,0.3,0.5,0.7,0.9,0.95,0.99]
        alpha = 0.9
        # for alpha in alphas:
        GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        result_gan, train_gan_time = GAN_.train(classifier_, alpha, epochs=2, data_original=data_original,
                                                batch_size=128, round=0)
        # del GAN_
        for round in np.arange(0, 2):
            data_[1] = (data_all[1][0][0:sum(data_size) + 1600], data_all[1][1][0:sum(data_size) + 1600])
            result_cls, roc, data_, classsifier, train_target_time = retrain_classifier(classifier_, GAN_, data_,
                                                                                        round=round)
            retrain_time = train_gan_time + train_target_time
            del GAN_
            classsifier_ = pickle.load(open('./SVM_all_ES_IAR/SVMATRound_' + str(round + 1) + '.sav', 'rb'))
            GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
            result_gan, train_gan_time = GAN_.train(classsifier_, alpha, epochs=2 + round * 10,
                                                    data_original=data_original, batch_size=128, round=round + 1)
            results.append(np.concatenate([[round + 1], result_cls, result_gan, [retrain_time]]))
            roc_curve_values.append(roc)
            file1 = open('SVM_all_ES_IAR/' + type, "w")
            # File_object.writelines(L for L in results)
            # print(results)

        plot_roc(roc_curve_values)
        Header = ['Round', 'Train_accuray', 'Test_accuray', 'Test_validationset_accuray', 'Train_FNR', 'Test_FNR',
                  'Test_validationset_FNR', 'accuracy', 'f1', 'precision', 'MCR on train data', 'MCR on test data',
                  'AvgD on train data', 'AvgD on test data', 'retrain_time']
        print("Writing data ...")
        with open('./SVM_all_ES_IAR/results.csv', 'w', newline='') as outfile1:
            wr = csv.writer(outfile1, delimiter=',', quoting=csv.QUOTE_NONE)
            wr.writerow([h for h in Header])
            for i in range(len(results)):
                wr.writerow(results[i])
            outfile1.close()


        # #del GAN_
        # #round=1
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=1)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train(alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=1)
        # results.append(np.concatenate([[1] ,result_cls, result_gan, [retrain_time]]))
        # #round=2
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=2)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train( alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=2)
        # results.append(np.concatenate([[2] ,result_cls, result_gan, [retrain_time]]))
        # #round=3
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=3)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train( alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=3)
        # results.append(np.concatenate([[3] ,result_cls, result_gan, [retrain_time]]))
        # #round=4
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=4)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train(alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=4)
        # results.append(np.concatenate([[4] ,result_cls, result_gan, [retrain_time]]))
        # #round=5
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=5)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train( alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=5)
        # results.append(np.concatenate([[5] ,result_cls, result_gan, [retrain_time]]))
        # #round=6
        # data_[1] = (data_all[1][0][0:sum(data_size)+1600], data_all[1][1][0:sum(data_size)+1600])
        # result_cls ,roc,data_,classsifier, train_target_time= retrain_classifier(classifier_, GAN_, data_, round=6)
        # retrain_time=train_gan_time+train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan,train_gan_time = GAN_.train( alpha, epochs=200+0*10, data_original=data_original, batch_size=128, round=6)
        # results.append(np.concatenate([[6] ,result_cls, result_gan, [retrain_time]]))
        # # round=7
        # data_[1] = (data_all[1][0][0:sum(data_size) + 1600], data_all[1][1][0:sum(data_size) + 1600])
        # result_cls, roc, data_, classsifier, train_target_time = retrain_classifier(classifier_, GAN_, data_, round=7)
        # retrain_time = train_gan_time + train_target_time
        # del GAN_
        # GAN_ = GAN(input_size=data_original[0][0].shape[1], hidden_size=200, noise_size=50)
        # result_gan, train_gan_time = GAN_.train(alpha, epochs=200 + 0 * 10, data_original=data_original,
        #                                         batch_size=128, round=7)
        # results.append(np.concatenate([[7], result_cls, result_gan, [retrain_time]]))
        #
        #
        #
        #
        # plot_roc(roc_curve_values)
        # Header = ['Round', 'Train_accuray', 'Test_accuray', 'Test_validationset_accuray' , 'Train_FNR', 'Test_FNR', 'Test_validationset_FNR'  ,'accuracy','f1','precision', 'MCR on train data' , 'MCR on test data' ,'AvgD on train data' , 'AvgD on test data', 'retrain_time']
        # print("Writing data ...")
        # with open('./SVM_manifest_ES_IAR/results.csv', 'w', newline='') as outfile1:
        #     wr = csv.writer(outfile1, delimiter=',', quoting=csv.QUOTE_NONE)
        #     wr.writerow([h for h in Header])
        #     for i in range(len(results)):
        #         wr.writerow(results[i])
        #     outfile1.close